{
  "hash": "8ec9e8363a987788ad3b2cd938a04ef0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Geospatial Machine Learning with ![](../img/Rlogo.png){width=60px}\"\nsubtitle: <span style=\"color:#2C3E50\">11.118/11.218 Applied Data Science for Cities</span>\ndate: \"Last Updated 2025-08-08\"\nformat: html\neditor: visual\nexecute: \n  warning: false\n---\n\n\n# Introduction\n\nLast week, we saw how machine learning can be used for prediction, and that different models vary in their predictive accuracy. More flexible models, like random forests, tend to capture complex patterns better. This week, we’ll take a closer look at tree-based models, including decision trees, ensembles, and boosting methods. The spatial element is added and we will learn why generalizability across space is so important for geospatial machine learning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(caret)\nlibrary(rattle)\nlibrary(gbm)\n```\n:::\n\n\n# Add Spatial Context to Housing Price Prediction\n\nWe’ll continue using the Boston housing price data to explore prediction models.\\\nLast time, we focused on building characteristics, following a typical hedonic model - a model that explains housing prices based on property features like size, age, or number of rooms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This part goes to the lecture\nboston <- read_csv(\"../data/boston_house_price_collapse.csv\")\n```\n:::\n\n\nHowever, houses exist in space, and housing prices are not randomly distributed. Real estate transactions often show a spatial pattern, where nearby houses tend to have similar prices. This is known as spatial autocorrelation, the idea that \"near things are more related than distant things.\"\n\nThis week, we’ll try including a variable that represents “nearby prices” to see if it increases the variance explained and improves our predictions. In the following code, we create a new variable that captures the average housing price in nearby neighborhoods, an idea known as **spatial lag**.\n\nConvert the dataset to a spatial feature using its longitude and latitude information:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston_sf <- \n  boston |> \n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) |> \n  st_transform(2249)\n```\n:::\n\n\nFor each house, draw a 500-meter buffer, identify all other houses within that buffer, and calculate their average price.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston_buffer <- st_buffer(boston_sf, dist = 1640) |> \n  select(Parcel_No, SalePrice)\n\nneighbors <- st_intersection(boston_sf, boston_buffer)\n\nave_price <- neighbors |>\n  filter(Parcel_No != Parcel_No.1) |> \n  group_by(Parcel_No) |>  \n  summarize(Ave_Neighbor = mean(SalePrice.1, na.rm = TRUE)) |>\n  st_drop_geometry() \n```\n:::\n\n\nThis average is then joined back to the original dataset as a new variable, giving each house a sense of its local price context.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston_with_splag <- boston_sf |>\n  left_join(ave_price, by = \"Parcel_No\")\n\n\nboston_with_splag <-  boston_with_splag |> \n  mutate(Ave_Neighbor = case_when(\n    is.na(Ave_Neighbor) ~ SalePrice,\n    TRUE ~ Ave_Neighbor\n  ))\n```\n:::\n\n\n# Fit a Decision Tree Model\n\nHere we repeat the modeling workflow from last time. First identify the variables we want to include. Then handle data issues such as rare categories and outliers. After cleaning the data, we split it into training and test sets to prepare for modeling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodeldata <- boston_with_splag |> \n  st_drop_geometry() |> \n  select(-c(Parcel_No, PricePerSq, LU, R_BLDG_STY))  |> \n  filter(Style != \"Unknown\") |> \n  filter(SalePrice < 5000000)\n\nset.seed(42)  # for reproducibility\n\ntrain_index <- createDataPartition(\n  y = modeldata$SalePrice,\n  p = .7,\n  list = FALSE\n)\n\ntrain_data <- modeldata[ train_index,]\ntest_data  <- modeldata[-train_index,]\n```\n:::\n\n\nAs we've seen with the `caret` package, we can fit different models by changing the `\"method\"` argument in `train()`. Here, we fit a basic decision tree model by setting `method = \"rpart\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_model <- train(\n  SalePrice ~ .,        \n  data = train_data,\n  method = \"rpart\",        \n  trControl = trainControl(\"cv\", number = 5),\n  tuneLength = 6\n)\n```\n:::\n\n\nCalculate the RMSE. Even with a simple decision tree, the prediction got much better than last time’s random forest model, just by adding the spatial lag variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict(tree_model, newdata = test_data) \ntree_result = RMSE(pred, test_data$SalePrice)\ntree_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 202977.8\n```\n\n\n:::\n:::\n\n\n### Feature Importance\n\n`varImp()` shows the importance of each variable in a model. In a tree-based model context, variable importance is measured by how much a variable reduces error (e.g., decreases node impurity) when used to split the data.\n\nOur spatial lag variable `Ave_Neighbor` ranks the highest among the predictors. The importance score is relative, meaning a variable scored 2.0 contributed twice as much a variable with a score of 1.0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_model_importance <- varImp(tree_model, scale = FALSE) \nplot(tree_model_importance, top = 5)\n```\n\n::: {.cell-output-display}\n![](lab9_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Tree structure\n\nA tree model is considered a “white-box” model, because we can easily see the if-then-else rules it uses to make predictions. At each split, the tree decides which variable to use and what value is appropriate for the split. Each path leads to a smaller group that is more homogeneous in terms of housing price.\n\nEach box typically shows three numbers: predicted value, % of total data, and number of observations. For instance, at #2 node, 571,982 is the average of the housing price variable in this group, n=1005 is the number of observations that fall in this group, and 97% means this number of observations takes up 97% of the entire dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfancyRpartPlot(\n  tree_model$finalModel,\n  main = \"Simple Decision Tree Model\",\n  sub = \"Boston Housing Sales\",\n  digits = -3\n)\n```\n\n::: {.cell-output-display}\n![](lab9_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Parameter Tuning\n\nA decision tree can technically keep growing until each leaf is pure (i.e., contains only one class or exact value). However, as the tree grows deeper, its splitting rules become increasingly specific. Initially, the splits capture meaningful patterns in the data, but later ones often reflect only noise. This leads to overfitting. To prevent this, we need to decide when to stop growing the tree.\n\nThe tree model includes an internal complexity parameter (cp), which serves as a form of regularization. This parameter adds a penalty for each additional split:\n\n-   A higher cp means more restriction (fewer splits, simpler tree).\n-   A lower cp allows more splits (risking overfitting).\n\nYou can specify `cp` directly in the `train()` function, or let caret automatically test a range of `cp` values to find the one that gives the best predictive performance.\n\nIn your `tree_model` results, you'll see a list of tested `cp` values. For example, we have set `tuneLength = 6`, so `caret` tried 6 different `cp` values. For each one, it uses 5-fold cross-validation to estimate the performance. After training, it reports: “The final value used for the model was cp = xxx”.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCART \n\n1036 samples\n  14 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 829, 828, 830, 829, 828 \nResampling results across tuning parameters:\n\n  cp          RMSE      Rsquared   MAE     \n  0.00943124  194878.8  0.8465393  130397.0\n  0.01244752  199303.3  0.8374178  134003.3\n  0.02585317  214115.0  0.8109520  143219.5\n  0.05757983  252965.7  0.7548125  153943.8\n  0.10196885  284744.4  0.6644264  178483.1\n  0.66917205  411661.0  0.5775574  226671.5\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.00943124.\n```\n\n\n:::\n:::\n\n\nThe `plot(tree_model)` displays the model performance (e.g., RMSE) for each tested value of `cp`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree_model)\n```\n\n::: {.cell-output-display}\n![](lab9_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n# From One Tree to a Forest\n\nIn 1906, statistician Francis Galton observed a contest to guess an ox's weight. There were 800 guesses, and while the individual guesses varied widely, the average was within 1% of the true weight. This “wisdom of crowds” idea, explored by James Surowiecki, also applies to predictive models: combining multiple models (an ensemble) is often more accurate than relying on just one.\n\nRandom forest is an **ensemble** model made up of many decision trees. Each tree makes its own prediction, and the final result is based on averaging (for regression) or majority vote (for classification). The randomness both in selecting data and variables helps reduce overfitting and improves the model’s ability to generalize to new data.\n\n\n![](../img/lab9_forest.jpg)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrf_model <- train(\n  SalePrice ~ .,        \n  data = train_data,\n  method = \"rf\",        \n  trControl = trainControl(\"cv\", number = 5),\n  tuneLength = 6\n)\n\npred <- predict(rf_model, newdata = test_data) \nrf_result <- RMSE(pred, test_data$SalePrice)\nrf_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 143326.3\n```\n\n\n:::\n:::\n\n\nWith the spatial variable, this random forest model produce better result (RMSE 143,326.3) compared with last week (RMSE \\~234,360).\n\n## **How was the forest formed?**\n\nRandom forest is built through two stages of random sampling. Each tree is trained on a random subset of the data, and at each split, each tree randomly selects a subset of predictors to consider. This added variation across trees mimics the wisdom of crowds: each tree sees a different part of the data and contributes its own “opinion,” leading to more stable and generalizable predictions.\n\n### **How many trees are there in a forest**\n\nIn the `caret` package, the number of trees in a random forest is controlled by the `ntree` parameter. You can pass it through the `train()` function. The default value is 500.\n\n\n\n```{.default}\ntrain(\n  ...\n  ntree = 500\n)\n```\n\n\n### **How many predictors each tree uses**\n\n`mtry` controls how many predictors are randomly selected and considered at each split in a tree. The random forest model in `caret` automatically tunes the `mtry` hyperparameter, meaning the model tests different values to see which one gives the best predictive performance.\n\nIn your model result below (`rf_model`), `mtry` shows the number of predictors randomly selected at each split of a tree. It tested 6 `mtry` values, because we have specified `tuneLength = 6` in the `train()` function. After the 6 attempts, the the model says \"The final value used for the model was mtry = 27.\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n1036 samples\n  14 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 828, 828, 829, 831, 828 \nResampling results across tuning parameters:\n\n  mtry  RMSE      Rsquared   MAE      \n   2    270273.4  0.7978104  150634.58\n   7    187092.3  0.8770591  106331.33\n  12    167880.8  0.8913759  100677.57\n  17    161211.3  0.8981539   99603.43\n  22    158843.8  0.8978341   99384.06\n  27    158736.9  0.8963383   99674.57\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 27.\n```\n\n\n:::\n:::\n\n\nThe `plot(rf_model)` displays the model performance (e.g., RMSE) for each tested value of `mtry`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(rf_model)\n```\n\n::: {.cell-output-display}\n![](lab9_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n### Hyperparameter tuning\n\nNow we begin to see that machine learning models come with **hyperparameters**. Models learn patterns from data, they can behave differently depending on how they're set up. A model can give you a random guess (underfitting)or try to accurately fit every single point (overfitting). What we want is a balance: a model that captures the real underlying patterns, but not the noise.\n\nHyperparameters are the \"knobs\" we use to tune that balance between accuracy and generalizability. Unlike parameters that are learned from the data (like coefficients in linear regression), hyperparameters are set by the users or selected through tuning (like `cp` in a decision tree, or `mtry` in a random forest).\n\nIf you're not satisfied with the default values that the model tries, or if you want to test specific values (such 2, 4, 6, 8), you can manually tune them by specifying a tuning grid using the `tuneGrid` argument in the `train()` function:\n\n\n\n```{.default}\n# Fit the model with your custom grid\nrf_model <- train(\n  ...\n  tuneGrid = expand.grid(mtry = c(2, 4, 6, 8)),\n\n)\n```\n\n\n## Two Ensemble Learning Methods\n\n**Bagging**\n\nWe mentioned each decision tree is trained on a different subset of the training data. This process is called **bagging**, short for **bootstrap aggregating**. It works by randomly sampling the original training data with replacement, meaning the same observation can be selected multiple times.\n\nThe statistical term bootstrapping comes from the phrase \"to pull oneself up by one’s bootstraps\", reflecting the idea that we generate many new datasets by resampling from the original data, without needing new data.\n\n**Boosting**\n\nWhile bagging builds many trees in parallel, **boosting** builds trees **sequentially**, where each new tree is trained to correct the mistakes made by the previous one. Both bagging and boosting are ensemble methods, they aggregate the outputs of multiple models, just in different ways\n\nIn boosting, the idea is to focus learning on the “hard” cases. The first tree might make many mistakes, and the second tree then tries to fix those mistakes by giving more weight to the misclassified or poorly predicted observations. This process continues, with each tree trained to minimize the residual error from the combined model so far.\n\n# **Fit A Boosted Tree**\n\nOne of the most commonly used boosting algorithms is the **Gradient Boosted Machine (GBM)**. GBM builds trees one at a time and each new tree is trained to predict the residuals (i.e., the remaining errors) from the current ensemble of trees.\n\n![](../img/lab9_gbm.png)\n\nUse the `method = \"gbm\"` for a Gradient Boosted Machine model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ngbm_model <- train(\n  SalePrice ~ .,        \n  data = train_data,\n  method = \"gbm\",        \n  trControl = trainControl(\"cv\", number = 5),\n  tuneLength = 3,\n  verbose = FALSE\n)\n\npred <- predict(gbm_model, newdata = test_data) \ngbm_result <- RMSE(pred, test_data$SalePrice)\ngbm_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 142774.8\n```\n\n\n:::\n:::\n\n\n## GBM Hyperparameters\n\n[This table](https://topepo.github.io/caret/available-models.html) shows all the models available in train(), along with their tunable parameters. For example, you can look up `rpart` and see that its tuning parameter is `cp`, and for `rf` (random forest), it's `mtry`.\n\nIf you scroll down to `gbm` (boosted tree model), you’ll notice it has four tuning parameters.\n\n### What are they\n\n-   **n.trees:** the number of trees in the (sequenced) ensemble (typical number: 50-500)\n-   **interaction.depth:** the maximum depth of each individual tree (usually start with 1-5)\n-   **shrinkage:** learning rate. how much each tree contributes to the final prediction. smaller value means slow learning. (Typical values: 0.01, 0.05, 0.1)\n-   **n.minobsinnode:** Minimum observations in terminal nodes. how small the leaf nodes (terminal nodes) can be. If this subpartition is trival, then stop it. Higher: less complicated trees. (Typical values: 10)\n\nWith more tuning knobs, it mean GBMs are prone to overfitting. It's tempting to adjust to fit the training data very well. If it fits the training data too well, it loses generalbility\n\n### How to tune\n\n**1.Let the model choose (automatic tuning):** We mentioned we can set up `tuneLength` in the `train` function. In the previous example, we have set `tuneLength = 3`, the model automatically selected 3 values for each of the most influential parameter.\n\nIn our case, both `n.trees` (number of trees) and `interaction.depth` (tree depth) were varied, each with 3 values. This led to a total of 3 × 3 = 9 combinations, which you can see in the result table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_model$results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  shrinkage interaction.depth n.minobsinnode n.trees     RMSE  Rsquared\n1       0.1                 1             10      50 219364.0 0.8420349\n4       0.1                 2             10      50 202008.2 0.8568887\n7       0.1                 3             10      50 193100.0 0.8633882\n2       0.1                 1             10     100 207917.4 0.8522571\n5       0.1                 2             10     100 188575.1 0.8659353\n8       0.1                 3             10     100 186397.0 0.8691328\n3       0.1                 1             10     150 203157.6 0.8555506\n6       0.1                 2             10     150 182642.5 0.8719976\n9       0.1                 3             10     150 183749.3 0.8717811\n       MAE   RMSESD RsquaredSD    MAESD\n1 123631.8 21014.13 0.07218021 6894.726\n4 113605.3 18676.79 0.05863182 3698.699\n7 108567.7 22721.83 0.05961338 2748.247\n2 117687.1 22009.86 0.06230836 4904.312\n5 110150.1 15023.60 0.05295433 2653.846\n8 106545.5 19020.91 0.05349108 3606.791\n3 117843.7 17405.58 0.05745362 3946.568\n6 108353.7 16029.87 0.04942192 4009.147\n9 106603.3 14395.91 0.04824818 3646.324\n```\n\n\n:::\n:::\n\n\n**2.Manual tuning (full control):** If you want more control, you can manually define the grid of values using `tuneGrid = expand.grid(...)` . The model will then try all possible combinations of the values you provide.\n\nIn the following code, I set up a grid where I will use a deeper tree (`interaction.depth`), and test it against 2 values of total number of trees (`n.trees`). This custom tuning improved our test RMSE.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_Grid <-  expand.grid(n.trees = c(50, 100),  \n                         interaction.depth = 5,     \n                         shrinkage = 0.1,   \n                         n.minobsinnode = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ngbm_model_tuned <- train(\n  SalePrice ~ .,        \n  data = train_data,\n  method = \"gbm\",        \n  trControl = trainControl(\"cv\", number = 5),\n  tuneGrid = gbm_Grid,\n  verbose = FALSE\n)\n\npred <- predict(gbm_model_tuned, newdata = test_data) \ngbm_tuned_result <- RMSE(pred, test_data$SalePrice)\ngbm_tuned_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 140898.6\n```\n\n\n:::\n:::\n\n\nAgain, we can see all the value combinations that the model used.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_model_tuned$results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  shrinkage interaction.depth n.minobsinnode n.trees     RMSE  Rsquared\n1       0.1                 5             10      50 194408.7 0.8634318\n2       0.1                 5             10     100 186034.2 0.8668271\n       MAE   RMSESD RsquaredSD    MAESD\n1 109854.7 21113.45 0.05437980 4906.340\n2 107333.1 15643.44 0.04977026 4106.315\n```\n\n\n:::\n:::\n\n\nHowever, trying too many combinations isn’t always necessary or helpful. Different models vary in how sensitive they are to tuning. Sometimes, it's perfectly fine to stop at a solution that is \"good enough\", especially when speed and simplicity matters.\n\n## GBM Feature Importance\n\nWe can also plot the variable importance of the GBM model using `varImp()`. The spatial lag variable still ranks among the most important predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_model_importance <- varImp(gbm_model_tuned, scale = FALSE) \n\n# We can also visualize the most important features in our model this way...\nplot(gbm_model_importance, top = 5)\n```\n\n::: {.cell-output-display}\n![](lab9_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n# Model Summary\n\nHere are the models we've tried today. Model complexity, when properly regularized and tuned, can lead to better predictive performance. But it's equally important to understand how these models actually work, so we can choose and apply them with purpose.\n\n| Model Type                             | RMSE                                                             |\n|----------------------------|--------------------------------------------|\n| Decision Tree                          | 202,977.8      |\n| Random Forest                          | 143,326.3        |\n| Gradient Boosted Machine               | 142,774.8       |\n| Gradient Boosted Machine (with tuning) | 140,898.6 |\n\n# Lab Report\n\nFor this homework, continue working with the Ames Housing dataset and improve the best model you built previously by adding spatial contexts and apply tree-based models.\n\n-   First, include a variable related to location information in your dataset. This could be a spatial lag variable, or a categorical neighborhood variable (as in Steif's book chapter).\n-   Then, apply both Random Forest and Gradient Boosted Tree models, adjusting a few key tuning parameters using either `tuneLength` or a custom `tuneGrid`. Try different settings to find the model that performs best based on test RMSE.\n\nWhat is the best model you’ve built after exploring different machine learning methods over the past two weeks? Take some time to reflect on your work and **prepare a report** sharing your findings.\n\nIn your report, tell us which model performed best based on test RMSE and explain why you think it stood out. Describe the different models you tried, what you liked or didn’t like about them, and any adjustments you made along the way like tuning parameters, selecting features, or creating variables. Please include any helpful tables or graphs that show your results, such as comparisons of RMSE values or visualizations that illustrate your model’s performance.\n\nKeep your report clear and straightforward so anyone reading it can easily follow your process and reasoning. We’re excited to see what you discovered!\n\nPlease prepare your report in a Quarto document and submit your Rendered HTML file to Canvas by the end of day, Tuesday, Sep 23.\n",
    "supporting": [
      "lab9_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}