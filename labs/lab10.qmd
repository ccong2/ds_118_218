---
title: "Fairness-Aware Machine Learning with ![](../img/Rlogo.png){width=60px}"
subtitle: <span style="color:#2C3E50">11.118/11.218 Applied Data Science for Cities</span>
date: "Last Updated `r Sys.Date()`"
format: html
editor: visual
execute: 
  warning: false
---

```{r}
library(tidyverse)
library(caret)
library(mlr3fairness)
```

# Data and Disputes

COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a popular commercial algorithm used by judges and parole officers for scoring criminal defendant’s likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants, and against black inmates, based on a two-year follow up study (i.e who actually committed crimes or violent crimes after 2 years) by [ProPublica](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm). The pattern of mistakes, as measured by precision/sensitivity is notable.

This dataset is frequently used to study machine learning bias and is not readily available through [`mlr3fairness`](https://search.r-project.org/CRAN/refmans/mlr3fairness/html/compas.html) package.

```{r}
data("compas", package = "mlr3fairness")
```

The `decile_score` column represents COMPAS’s predicted risk of recidivism, ranging from 1 to 10. The `score_text` column maps these scores into categories: scores from 1 to 4 are labeled as Low risk, 5 to 7 as Medium risk, and 8 to 10 as High risk.

The `two_year_recid` column indicates whether the defendant was rearrested within two years.

To simplify our analysis, we’ll create two new columns. The first, **predicted**, will be "Yes" if `score_text` is High or Medium, and "No" if it is Low. The second, **actual**, will be "Yes" if `two_year_recid` is 1 and "No" if it is 0.

```{r}
# Data preprocess
compas <- compas |> 
  mutate(
    predicted = case_when(
      score_text %in% c("High", "Medium") ~ "yes",
      score_text %in% c("Low") ~ "no"
    ),
    actual = factor(two_year_recid, 
                    levels = c(0, 1), 
                    labels = c("no", "yes"))
  )
```

## **Model Accuracy**

How well does COMPAS predict reoffending? One way to measure this is by checking the model’s accuracy. Since this is a classification problem with "Yes" or "No" outcomes, accuracy is defined as the proportion of correct predictions: both when the model predicts reoffending and the person does reoffend, and when it predicts no reoffending and the person does not.

In other words, accuracy tells us how often the model is correct overall.

```{r}
correct_predictions <- compas |>
  filter(
    (predicted == "yes" & actual == "yes") |
    (predicted == "no" & actual == "no")
)

accuracy <- nrow(correct_predictions) / nrow(compas)
accuracy
```

In our case, the accuracy of the COMPAS model is about 0.66, meaning it makes the correct prediction 66% of the time. It's better than random guessing (50%), although only moderately.

## Confusion Matrix

Accuracy is not the best metrics for evaluating classification models, as it doesn’t show how it makes mistakes. It treats a **false positive** (predicting someone will reoffend when they won’t) the same as a **false negative** (predicting someone won’t reoffend when they will). In practice, these errors can have very different consequences.

Confusion matrix breaks down the types of correct and incorrect predictions:

```{r}
# True Positive: predicted recidivism, actually recidivated
TP <- sum(compas$predicted == "yes" & compas$actual == "yes")  

# True Negative: predicted no recidivism, didn't recidivate
TN <- sum(compas$predicted == "no" & compas$actual == "no")  

# False Positive: predicted recidivism, didn't recidivate
FP <- sum(compas$predicted == "yes" & compas$actual == "no")  

# False Negative: predicted no recidivism, actually recidivated
FN <- sum(compas$predicted == "no" & compas$actual == "yes")  

# Create a matrix manually
conf_matrix <- matrix(c(TN, FN, FP, TP),
                      nrow = 2,
                      dimnames = list(Prediction = c("N", "P"),
                                      Reference = c("N", "P")))
conf_matrix
```

This matrix reads:

```         
           Reference (Actual)
Prediction     0      1
         -----------------
      0 |   TN     FN
      1 |   FP     TP
```

Recall that a perfect classifier would only have non-zero values on it's main diagonal (TN and TP).

To put this result in our context, among the 3,421 (first column) people who actually did not reoffend, 1,076 were misclassified. That’s about 31.5%, or nearly one-third of non-reoffenders were flagged as high risk. This is called the **False Positive Rate (FPR)**: the proportion of non-reoffenders incorrectly predicted as reoffenders.

## Breaking down by racial groups

If we take a closer look and break down the confusion matrix by racial groups, we will see that the model’s errors are not evenly distributed.

[The analysis by Propublica](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) has this quote:

> "Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts"

This means the **FPR**—predicting someone will reoffend when they won’t—is significantly higher for Black defendants compared to white defendants.

We’ll now reproduce this analysis ourselves.

To make this easier and reusable, we’ll define a function that generates a confusion matrix for any filtered dataset.

```{r}
# Define a function to calculate confusion matrix cells by race
confusion_by_race <- function(df, race_name) {
  df_race <- df %>% filter(race == race_name)
  
  TP <- sum(df_race$predicted == "yes" & df_race$actual == "yes")
  TN <- sum(df_race$predicted == "no" & df_race$actual == "no")
  FP <- sum(df_race$predicted == "yes" & df_race$actual == "no")
  FN <- sum(df_race$predicted == "no" & df_race$actual == "yes")
  
  return(c(TP = TP, TN = TN, FP = FP, FN = FN))
}

```

Then calculate the confusion matrix for Black and White population, separately.

```{r}
# Remove other objects from the workspace to start fresh
rm(list = setdiff(ls(), c("compas", "confusion_by_race")))

# Calculate for Black affenders
b_vals <- confusion_by_race(compas, "African-American")
b_fpr <- b_vals["FP"]/(b_vals["FP"]+b_vals["TN"])

# Calculate for White affenders
w_vals <- confusion_by_race(compas, "Caucasian")
w_fpr <- w_vals["FP"]/(w_vals["FP"]+ w_vals["TN"])

# Print our conclusions
str_glue(
  "Black defendants who did not recidivate over a two-year period ",
  "were nearly twice as likely to be misclassified as higher risk ",
  "compared to their white counterparts ({round(b_fpr*100)}% vs. {round(w_fpr*100)}%).")
```

Propublica has another quote:

> "White defendants were often predicted to be less risky than they were. Our analysis found that white defendants who re-offended within the next two years were mistakenly labeled low risk almost twice as often as black re-offenders"

This points to another type of model error: **False Negative Rate (FNR)**. It tells us, among all people who actually reoffended (the actual positives), how many were missed by the model (predicted as low risk).

Going back to our matrix, **FNR** is saying, out of all the actual reoffenders (second column), how many were incorrectly predicted as "no" (FN)

```         
           Reference (Actual)
Prediction     0      1
         -----------------
      0 |   TN     FN
      1 |   FP     TP
```

```{r}
# Calculate for Black reaffenders
b_fnr <- b_vals["FN"]/(b_vals["FN"]+b_vals["TP"]) 

# Calculate for White reaffenders
w_fnr <- w_vals["FN"]/(w_vals["FN"]+ w_vals["TP"])

str_glue(
  "White defendants who reoffend were given more lenient risk scores. \n",
  "{round(w_fnr*100)}% of White defendants were wrongly labeled as low risk compared with {round(b_fnr*100)}% for Black defendants.")
```

Steif's book refers to this kind of pattern as **disparate impact**: when a decision-making process seems neutral on the surface but still results in unequal outcomes for certain groups. In this case, the COMPAS model tends to over-predict risk for Black defendants (higher False Positive Rate) and under-predict for White defendants (higher False Negative Rate), leading to unequal treatment, even though race is not explicitly used as a factor.

Does this mean the COMPAS model simply wasn’t effective? To explore this further, let’s try building our own prediction model using logistic regression and see how well it performs.

# Logistic Regression

## **Variable Selection**

Choosing which variables to include in a regression model is a critical challenge in algorithmic decision-making. Steif’s book highlights an important tension around using features like `race` or `prior_count`.

If we include race in a multiple linear regression (MLR), the results table will show the effect of the `race` variable, which might read: “Being race X increases predicted recidivism by 0.2”. This kind of output is problematic. It means the model will consistently add 0.2 for defendants of race X. In other words, it encodes bias into the predictions.

Even if you remove `race` as a predictor, the model may still "learn" patterns related to race, because things like `priors_count` are correlated with `race` due to historical and systemic biases in policing. Similarly, things such as redlining neighborhoods and credit scores, which are closely connected to race or income due to historical segregation and unequal access to resources, can lead algorithms to systematically make worse predictions for people from certain socioeconomic groups.

What we can do to address this issue is broadly referred to as **Fairness-Aware Modeling**. This includes a range of strategies: understanding the data deeply, being cautious about variables that may act as proxies for protected characteristics, and applying fairness techniques such as adjusting decision thresholds. We’ll try to build and improve a logistic regression model using some of these ideas.

## **Model Building**

We’ll now repeat the modeling workflow you’ve used in the past two classes. We’ll exclude `race` from the predictors, but include `priors_count`, while acknowledging that it may act as a problematic proxy for race.

```{r}
modeldata <- compas 

set.seed(42)
train_index <- createDataPartition(
  y = modeldata$two_year_recid,
  p = 0.7,
  list = FALSE
)

train_data <- modeldata[train_index, ]
test_data  <- modeldata[-train_index, ]
```

In the `train()` function, we set `method = "glm"` to specify that we’re using a generalized linear model. `family = "binomial"` indicates that the outcome variable is binary. The `trainControl()` function is set up as before to perform five-fold cross-validation.

```{r}
set.seed(42)
logit_model <- train(
  actual ~ age + sex + c_charge_degree + priors_count + length_of_stay, 
  data = train_data,
  method = "glm",        
  family = "binomial",
  trControl = trainControl(method = "cv", number = 5)
)
```

## **Our Result**

Now we use the trained model to predict the classes for the test dataset.

```{r}
pred_class <- predict(logit_model, newdata = test_data)
```

After generating these predicted classes, we attach them back to the test dataset for the subsequent confusion matrix calculation.

```{r}
# attach to the test data
test_pred_class <- test_data %>%
  mutate(predicted = pred_class)
```

Reuse our previous function `confusion_by_race` to calculate FPR by race:

```{r}
b_vals <- confusion_by_race(test_pred_class, "African-American")
b_fpr <- b_vals["FP"]/(b_vals["FP"]+b_vals["TN"])

w_vals <- confusion_by_race(test_pred_class, "Caucasian")
w_fpr <- w_vals["FP"]/(w_vals["FP"]+ w_vals["TN"])

str_glue(
  "Black defendants who did not recidivate but misclassified as higher risk: {round(b_fpr*100)}%. \n",
  "White defendants who did not recidivate but misclassified as higher risk: {round(w_fpr*100)}%).")
```

Although our model has lower errors rate than COMPAS, it produces very similar results and shows the same kind of disparities that are roughly twice as large.

## Threshold Tuning

We can’t undo the historical patterns that shaped the data, and we often can’t completely avoid using variables that carry implicit bias, especially they carry predictive power. So what can we do?

In logistic regression, the model doesn’t directly predict a “yes” or “no” outcome. It predicts a probability between 0 and 1 that an observation belongs to the positive class. A final classification decision still needs to be made. By default, the threshold is set at 0.5, meaning if the predicted probability is greater than 0.5, the case is classified as “yes” (positive), otherwise “no” (negative).

By setting `type = "prob"` in the following code, we get **predicted probabilities** instead of class labels. This lets us adjust the decision threshold. For example, classifying a case as "yes" only if the probability is above 0.7, rather than the default 0.5. Changing the threshold affects how predictions fall into true or false positives and negatives, which in turn impacts error rates and fairness.

```{r}
# This is the probability result
pred_probs <- predict(logit_model, newdata = test_data, type = "prob")
```

Now let's set different classification thresholds for different groups!

```{r}
# Attach pred_probs back to test data
test_pred_prob <- test_data %>%
  mutate(pred_prob = pred_probs$yes)

# Define threshold we want to use
thresholds <- list(
  African_American = 0.7,
  Caucasian = 0.6
)

# Create a new "predicted" column based on group-specific thresholds.
# For African-American and Caucasian individuals, apply separate thresholds.
# For all others, use the default threshold of 0.5.
test_pred_prob <- test_pred_prob %>%
  mutate(predicted = case_when(
    race == "African-American" & pred_prob > thresholds$African_American ~ "yes",
    race == "African-American" & pred_prob <= thresholds$African_American ~ "no",
    race == "Caucasian" & pred_prob > thresholds$Caucasian ~ "yes",
    race == "Caucasian" & pred_prob <= thresholds$Caucasian ~ "no",
    pred_prob > 0.5 ~ "yes",
    TRUE ~ "no"
  ))

# three metrics, FNR, FPR and Accuracy for both race

```

Now that we’ve applied group-specific thresholds, we calculate and compare performance metrics: **False Positive Rate (FPR)**, **False Negative Rate (FNR)**, and **Accuracy**, for African-American and Caucasian defendants.

```{r}
b_vals <- confusion_by_race(test_pred_prob, "African-American")
b_fpr <- b_vals["FP"] / (b_vals["FP"] + b_vals["TN"])
b_fnr <- b_vals["FN"] / (b_vals["FN"] + b_vals["TP"])
b_acc <- (b_vals["TP"] + b_vals["TN"]) / sum(b_vals)

w_vals <- confusion_by_race(test_pred_prob, "Caucasian")
w_fpr <- w_vals["FP"] / (w_vals["FP"] + w_vals["TN"])
w_fnr <- w_vals["FN"] / (w_vals["FN"] + w_vals["TP"])
w_acc <- (w_vals["TP"] + w_vals["TN"]) / sum(w_vals)

# Print results
str_glue(
  "Black Defendants:\n",
  "  FPR: {round(b_fpr * 100, 1)}%\n",
  "  FNR: {round(b_fnr * 100, 1)}%\n",
  "  Accuracy: {round(b_acc * 100, 1)}%\n"
)

str_glue(
  "White Defendants:\n",
  "  FPR: {round(w_fpr * 100, 1)}%\n",
  "  FNR: {round(w_fnr * 100, 1)}%\n",
  "  Accuracy: {round(w_acc * 100, 1)}%\n"
)
```

Steif's book introduced a few guildlines for an "equity threshold",

-   Minimize differences in False Positive Rates (FPR) and False Negative Rates (FNR) between racial groups, while keeping both rates relatively low.
-   Minimize differences in Accuracy rates between racial groups, while keeping the overall Accuracy high.

How would you interpret our new results? Would you consider this a better and more equitable model, or not? As discussed in the book, expect trade-offs: no single threshold will perfectly balance accuracy and fairness across all groups, which reflects real-world complexity in social data.

# Lab Report

Building on what we’ve learned so far, you will continue exploring how adjusting decision thresholds can help build fairness-aware machine learning models.

Previously, we tested one possible pair of thresholds (0.7 for Black defendants and 0.6 for White defendants) to see how it affected fairness metrics. But that was just one possibility. **Are there other threshold pairs that might lead to better fairness or overall performance? How would you find them?**

One powerful approach is to use sensitivity analysis: systematically testing a range of threshold values (e.g., from 0.1 to 1.0) for both groups and observing how model behavior changes.

### **Your Task**

Craft a **strategy** to explore and compare different threshold combinations. Think like a data scientist advising a team of stakeholders: you’ll need to experiment, visualize, interpret, and explain.

### **What to Do**

1.  **Design your approach.**\
    Decide how you want to vary the thresholds (e.g., in 0.1 increments from 0.1 to 1.0) and how many combinations to test.
2.  **Run your analysis.**\
    For each pair, calculate FPR, FNR, and Accuracy for both racial groups. You can write a loop to do this efficiently.
3.  **Communicate your findings.**\
    Use both **tables** and **visualizations** to show how the metrics change. Good options include:
    -    A results **table** showing values across threshold pairs

    -   **Line plots** or **heatmaps** to reveal patterns and trade-offs visually
4.   **Reflect and explain.**\
    Choose a recommended threshold pair based on your analysis. Please include a 300-500 word write-up to explain what you recommend and why, any trade-offs involved and how you’d explain it to a general audience.

There’s no perfect answer. This is about exploring, reasoning, and practicing fairness-aware modeling in action.

Please include all your work in a Quarto document and submit your Rendered HTML file to Canvas by the end of day, Tuesday, Sep 23.
