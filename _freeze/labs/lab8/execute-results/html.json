{
  "hash": "240ea783c9db839407a7dfbc34fd9ea1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multivariate Regression and Model Selection with ![](../img/Rlogo.png){width=60px}\"\nsubtitle: <span style=\"color:#2C3E50\">11.118/11.218 Applied Data Science for Cities</span>\ndate: \"Last Updated 2025-08-08\"\nformat: html\neditor: visual\nexecute: \n  warning: false\n---\n\n\n# Introduction\n\nThis week begins our journey into machine learning. Ken Steif’s [book chapter](https://urbanspatial.github.io/PublicPolicyAnalytics/intro-to-geospatial-machine-learning-part-1.html) walks through an example using Boston housing data, showing how to build, apply, and evaluate a predictive model for home prices. We will build our study on this example, and continue to explore more models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(caret)\n```\n:::\n\n\n# Data Processing\n\n## Get to know the variables\n\nWe’ll be working with the CSV dataset containing sale prices and housing characteristics for homes sold in Boston between August 2015 and August 2016. The data used in this tutorial is a subset of a larger dataset assembled from [here](https://data.boston.gov/dataset/property-assessment). You can explore the full [data dictionary here](https://data.boston.gov/dataset/property-assessment/resource/dbdc1bd8-60af-4913-a788-5f91cb68541b).\n\nRead in the dataset using `read_csv()` and take an initial look using functions like `glimpse()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston <- read_csv(\"../data/boston_house_price_collapse.csv\")\n#head(boston)\n```\n:::\n\n\nA few notes:\n\n-   The `SalePrice` variable is the target we want to predict.\n-   Other columns describe building characteristics — such as square footage, number of floors, and number of bedrooms.\n-   Some variables are numeric (`dbl`), while others are categorical (`chr` or `fct`).\n\n**Question:** Based on your first observation, which variables do you think might be strong predictors of house price? Which ones seem less useful?\n\n## **EDA and data processing**\n\nBefore feeding any data into a model, we need to understand and clean it. Every dataset is different, but there are some key steps you should always consider:\n\n1.  Understand data structure: `str()`, `glimpse()` are useful to get to know the details of variables we are dealing with. Numeric and categorical variables require different treatment in a model later on so this is something to keep in mind. We will also see in our case, `parcel_no` is just an identifier and is not useful for predicting price.\n\n2.  Handle missing values. `is.na()`, `sum(is.na(...))`, `colSums(is.na(...))` can help identify which variables have missing values and how many. But how will deal with them (drop, fill, or flag) will depend on variable contexts and further inspection.\n\n3.  Look at distributions. This is an important step before fitting a regression model because it helps guide feature selection and transformation. We will take a closer look at how these relationships show up in different types of variables:\n\n    ### Continuous variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston |> \n  select(SalePrice, PricePerSq, LivingArea, GROSS_AREA, NUM_FLOORS, R_BDRMS) |> \n  pairs()\n```\n\n::: {.cell-output-display}\n![](lab8_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe `pairs()` function creates a pairwise scatter plot matrix, which lets you visually examine the relationships between multiple variables at once. In the graph, we usually look for the following:\n\n1.  **Top row** (`SalePrice` as Y-axis):These show how each predictor relates to the outcome. If you see an upward trend (a roughly linear increase with `SalePrice`), that predictor is likely to be a strong predictor of price.\n\n2.  **Relationships between predictors:** Look for predictors that are highly correlated with each other (a sign of multicollinearity). For instance, `LivingArea` and `GrossArea` may appear nearly linear in their scatterplot. If you calculate the Pearson correlation, it's about 0.94. This means they contain overlapping information.\n\n    -   Depending on the modeling method (e.g., linear regression vs. tree-based methods), multicollinearity may or may not be a serious concern, but it’s always worth noting.\n\n3.  **Pattens, outliers and clusters.** These visual cues in pairwise plots help you decide whether a linear model is appropriate, and whether certain data points might influence your results. For example:\n\n    -   The variable `PricePerSq` may look odd with a bunch of y values at x=0. `PricePerSq` here is a derived variable, calculated as `SalePrice / LivingArea`. Since it's directly related to the outcome, it should not be used as a predictor of `SalePrice` .\n\n    -   You’ll also notice clear outliers especially for luxury homes that have extremely large sale prices. This should match what you would see in the histogram of `SalePrice`. Most homes are priced under \\$1 million, but a small number of high-end homes stretch the distribution far to the right.\n\n### Categorical variables\n\nYou can also try making scatterplots with categorical variables (e.g., by including them in a `pairs()` plot). But the results often look strange or uninformative, because points just line up vertically at two or three x-values. A better approach is to compare the average outcome across the categories. For example, you can group the data by a variable like `Style` and compute the mean `Saleprice` for each category:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston |> \n  select(SalePrice, Style, OWN_OCC, R_BLDG_STY, R_ROOF_TYP) |> \n  filter(SalePrice <= 1000000) |> \n  pivot_longer(-SalePrice, names_to = \"Variable\", values_to = \"Value\") |>  \n  group_by(Variable, Value) |> \n  summarise(mean_price = mean(SalePrice, na.rm = TRUE)) |> \n  \n  ggplot() +\n    geom_col(aes(x = Value, y = mean_price),position = \"dodge\") +\n    facet_wrap(~ Variable, scales = \"free_x\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(y = \"Mean Sale Price\", x = \"Category\", title = \"Mean SalePrice by Categorical Variables\")\n```\n\n::: {.cell-output-display}\n![](lab8_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWhen examining categorical variables, we’re not looking for a linear trend, because the categories don’t have a natural order or numeric spacing. Instead, we focus on two main things:\n\n1.  **Rare or problematic categories**. Just like with missing values in numeric variables, rare categories can cause problems in modeling. For example, if we look at the `Style` variable, we might see a category labeled \"Unknown\" and it only appears twice in the dataset. Such categories don’t offer enough data to reliably estimate their effect, may introduce noise, and might need to be removed or combined.\n\n2.  **Differences between groups**. We want to know if the categories differ meaningfully in the outcome variable. If all the categories have similar average sale prices, it means the variable doesn’t explain much of the variation in price and is likely a weak predictor. In our case, `OWN_OCC` seems to have little impact on sale price.\n\n# **Build a basic linear model**\n\nWe have found a few flags. Let’s first remove a few variables that look redundant or irrelevant, and try a simple linear model to see how things look.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodeldata <- boston |> \n  select(-c(Parcel_No, PricePerSq, LU, R_BLDG_STY,\n                                  Latitude, Longitude))  \n\nlm_model <- lm(data = modeldata, SalePrice ~ .)\nsummary(lm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = SalePrice ~ ., data = modeldata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1581568  -166556     1954   151317  6587294 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         403514.45  195844.15   2.060 0.039539 *  \nLivingArea             561.47      46.15  12.167  < 2e-16 ***\nStyleColonial      -194193.34   50285.64  -3.862 0.000117 ***\nStyleConventional  -118739.98   57914.28  -2.050 0.040517 *  \nStyleDecker         -68635.51   81578.33  -0.841 0.400293    \nStyleOther           10631.17   54324.84   0.196 0.844875    \nStyleRow_End        150699.85   76959.61   1.958 0.050401 .  \nStyleRow_Middle     354156.20   78259.10   4.525 6.52e-06 ***\nStyleSemi_Det       -79913.07   73357.63  -1.089 0.276174    \nStyleTwo_Fam_Stack   18779.30   63061.78   0.298 0.765904    \nStyleUnknown       -510897.38  283123.23  -1.805 0.071359 .  \nStyleVictorian     -403453.48  114498.84  -3.524 0.000439 ***\nGROSS_AREA            -185.93      28.24  -6.583 6.41e-11 ***\nNUM_FLOORS          183096.26   38356.10   4.774 1.99e-06 ***\nR_BDRMS             -33113.21   11177.31  -2.963 0.003101 ** \nR_FULL_BTH          203714.47   21610.53   9.427  < 2e-16 ***\nR_HALF_BTH           82726.93   21699.68   3.812 0.000143 ***\nR_KITCH            -264426.16   32033.43  -8.255 3.37e-16 ***\nR_FPLACE            176888.99   16043.91  11.025  < 2e-16 ***\nOWN_OCCY             19675.45   23567.76   0.835 0.403941    \nR_ROOF_TYPG         170755.27   42921.53   3.978 7.28e-05 ***\nR_ROOF_TYPH         157977.62   50487.71   3.129 0.001789 ** \nR_ROOF_TYPL         164093.91   75333.10   2.178 0.029548 *  \nR_ROOF_TYPM         334724.05   54660.45   6.124 1.17e-09 ***\nR_ROOF_TYPS         315239.92  203916.34   1.546 0.122339    \nR_ROOF_TYPU         321580.85  187673.74   1.714 0.086831 .  \nR_TOTAL_RM          -19855.50    8206.89  -2.419 0.015669 *  \nYR_BUILT              -298.72      98.63  -3.029 0.002500 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 396300 on 1457 degrees of freedom\nMultiple R-squared:  0.5784,\tAdjusted R-squared:  0.5705 \nF-statistic: 74.02 on 27 and 1457 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## Model Interpretation\n\n1.  **Coefficients:** Effect Size\n\nThe `Estimate` tell us how each predictor is associated with `SalePrice`, assuming all other variables are held constant. The sign of the values (+ or -) tell you the direction of the relationship. The `Standard Error` is about how certain we are about the estimate, smaller values mean more confidence.\n\n-   Then we might see a few issues: `GROSS_AREA` is negatively associated with housing price, so is `R_BDRM`, which doesn’t make intuitive sense. This relates back to the multicollinearity issue we've spotted. When this happens, the model struggles to disentangle their individual effects.\n\nFor continuous variables, the values of `Estimate` represent how much `SalePrice` changes for a 1-unit increase in the predictors. If `LivingArea` has a coefficient estimate of 500, it means for each additional square foot, price increases by \\$500, assuming everything else stays the same.\n\nFor categorical variables, one category is used as the reference group, and the others are compared against it. For example, `Style` includes 11 types of housing style, R will pick one (like \"Cape\") as the base. if the coefficient for `\"Colonial\"` is -190,000, it means homes with the `\"Colonial\"` style are predicted to cost **\\$190,000 less** than `\"Cape\"` style homes, on average.\n\n2.  **Coefficients:** Significance\n\nThe `t-statistics` and its mirror image, the `p-value`, measures the extent to which a coefficient is \"statistically significant\", in other words, whether the relationship we see between that predictor and the outcome is likely real, or could have happened just by random chance.\n\nThe higher that t-statistics (and the lower the p-value), the more significant the predictor. Since parsimony is a valuable model feature, it is useful to have a tool like this to guide choice of variables to include as predictors.\n\n3.  **Model performance**\n\nModel performance means how well our regression model fits the data and how useful it is for making predictions. We have several key metrics to evaluate this:\n\nThe **coefficient of determination (R²)** tells us the proportion of variance in the outcome that is explained by the predictors, ranging from 0 (no explanatory power) to 1 (perfect fit).\n\nThe **residual standard error (RSE)** measures the average size of the prediction errors of the outcome. Lower values indicate a better fit.\n\nThe **model p-value**, derived from an F-test, tells us whether the model as a whole is statistically significant, i.e. whether it does better than a model with no predictors.\n\nWith that, we can see our linear model explains 57% of the variation in sale prices, which is a decent but not great fit. On average, data points are \\$396,300 away from the values on the regression line.\n\n# **Moving to Machine Learning**\n\nTraditionally, the primary use of regression was to **understand and explain** the relationship between variables. The main focus was on the estimated parameters, which tell us how each predictor is associated with the outcome.\n\nWith the advent of big data, regression is increasingly used not just to explain, but to **predict** - building models that can forecast outcomes for new data. Alongside regression, many machine learning methods are now also used to improve predictive accuracy.\n\n-   **Traditional statistical methods** might ask:\\\n    *\"How does LivingArea affect SalePrice? Is the effect statistically significant?\"*\n\n-   **Machine learning** asks:\\\n    *\"Can we accurately predict SalePrice given LivingArea and 10 other features?\"*\n\nThe three model performance metrics we looked at earlier (R-square, RSE, p-value) show how well our model **fits** the existing data. They’re based on residuals, which measure how far the regression line is from the actual data points. However, these metrics don’t tell us how well the model will **predict** on new, unseen data.\n\nTo evaluate the model’s predictive power, we need to:\n\n1.  **Apply the model to a new dataset.** In machine learning, we usually split the data into a **training set** and **a test set** for this purpose. We train the model on one portion of the data, and test it on the rest to see how well it generalizes.\n\n2.  **Compare the predicted values with the actual values in the test set.** The primary metric we use is **RMSE** (Root Mean Squared Error). RMSE measures the average difference between the predicted values and the actual values. This The lower the RMSE, the better the model’s predictive accuracy.\n\n## **Machine Learning Workflow**\n\nI will go through a full modeling workflow using the `caret` package. `caret` lets you build many models (`lm`, `glmnet`, `rf`, `svm`, `xgbTree`, etc.) using a consistent syntax.\n\n**Step 1:** Preprocessing\n\n-   Remove irrelevant columns, such as IDs.\n-   Drop or combine categories with very few cases.\n-   Handle outliers, especially for skewed variables like SalePrice.\n    -   Option 1: Drop extremely high values (e.g., over \\$5M).\n    -   Option 2: Log-transform SalePrice, but that changes interpretation. For simplicity, we’ll just drop a few extreme outliers here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodeldata <- boston |> \n  select(-c(Parcel_No, PricePerSq, LU, R_BLDG_STY,\n                                  Latitude, Longitude))  |> \n  filter(Style != \"Unknown\") |> \n  filter(SalePrice < 5000000)\n```\n:::\n\n\n**Step 2:** Split data into training and testing sets\n\nThis involves first applying `createDataPartition` to generate row indices for the training set, then using those row indices to isolate the two subsets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set a fix \"random seed\" to make the split reproducible\nset.seed(42)  \n\n# Generate row indices for the training set\ntrain_index <- createDataPartition(\n  y = modeldata$SalePrice, # stratify based on the SalePrice variable\n  p = 0.7,                 # we want 70% of the data in the training set.\n  list = FALSE             # makes sure the output is a vector, not a list, so it's easier to use for indexing.\n)\n\n# Isolate the two subsets\ntrain_data <- modeldata[ train_index,]\ntest_data  <- modeldata[-train_index,]\n```\n:::\n\n\n**Step 3:** Build the model\n\nIn the following code, you can swap out the model by simply changing the argument for `method=` , such as linear regression (`\"lm\"`), random forest (`\"rf\"`), elastic net (`\"glmnet\"`), XGBoost (`\"xgbTree\"`), etc.\n\nAnother important point in this model is that we’ve set up cross-validation using `trainControl()`. Cross-validation means we're splitting the `train_data` into smaller parts **during training** to better evaluate model performance. Specifically, with 5-fold cross-validation (`number = 5`):\n\n-   Each time, one part of the data is set aside as a **validation set**, and the model is trained on the remaining parts.\n-   Then, the model is tested on that validation set.\n-   This process is repeated 5 times so that every fold (or subset) gets a turn as the validation set.\n\nWhy do we do this? Because it helps us avoid results that are just due to random chance from a single split. Finally, the validation results from all 5 folds are averaged, which gives us a more stable and reliable estimate of model performance before we even touch the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model <- train(\n  SalePrice ~ .,                \n  data = train_data,\n  method = \"lm\",          # this is where you change model type\n  trControl = trainControl(method = \"cv\", number = 5)\n)\n```\n:::\n\n\n**Step 4:** Evaluate Model Accuracy Using RMSE\n\nNow we evaluate how well our model performs on unseen data, that’s what the test set is for.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use predict() to generate predicted sale prices for the test set.\npred <- predict(lm_model, newdata = test_data) \n\n# compare those predictions to the actual sale prices using RMSE\nlm_result <- RMSE(pred, test_data$SalePrice)\n\nlm_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 269617\n```\n\n\n:::\n:::\n\n\n`RMSE` is calculated by compare the predicted and actual values in the test data, square it, find the mean, then take the square root, you have do it by hand like the following code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(mean((test_data$SalePrice - pred)^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 269617\n```\n\n\n:::\n:::\n\n\n## **Regularization: Controlling Model Complexity**\n\nIt might seem like a good idea to include as many variables as possible in our linear model, but that’s not always the best approach. When we include too many predictors, the model can start to overfit, meaning it fits the training data too closely and ends up capturing random noise rather than meaningful patterns.\n\nRegularization, in its broad sense, is a way to keep models simpler and more generalizable. For linear models, regularization helps the model to \"selectively use\" only the most important predictors.\n\nThere are two common types of linear model regularization: **Ridge regression** shrinks the coefficients of less important variables. **Lasso regression** shrinks some coefficients all the way to zero, effectively dropping those variables from the model.\n\nI'm repeating and putting the four steps together in the following code. The only difference is in the model-building step. Different models require their own set of tuning parameters, which we’ll explore more next week.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodeldata <- boston |> \n  select(-c(Parcel_No, PricePerSq, LU, R_BLDG_STY,\n                                  Latitude, Longitude))  |> \n  filter(Style != \"Unknown\")|> \n  filter(SalePrice < 5000000)\n\nset.seed(42)  # for reproducibility\n\ntrain_index <- createDataPartition(\n  y = modeldata$SalePrice,\n  p = 0.7,\n  list = FALSE\n)\n\ntrain_data <- modeldata[ train_index,]\ntest_data  <- modeldata[-train_index,]\n\nlasso_model <- train(\n  SalePrice ~ ., \n  data = train_data,\n  trControl = trainControl(method = \"cv\", number = 5),\n  method = \"glmnet\",\n  tuneLength = 10,\n  preProcess = c(\"center\", \"scale\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict(lasso_model, newdata = test_data) \nlasso_result <- RMSE(pred, test_data$SalePrice)\nlasso_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 268423.9\n```\n\n\n:::\n:::\n\n\n## Non-linear model\n\nSo far we have been working with linear models, But sometimes, the data just doesn’t fit a straight line (or a flat plane, if we’re in a space with many variables). Real-world relationships can be nonlinear, irregular, or involve interactions between variables that a linear model can’t easily capture.\n\nWe will introduce Random Forest model next time, but it is one of the machine learning models that do not assume a straight-line relationship between predictors and the outcome (non-linear model), and do not rely on a fixed number of parameters (non-parametric model). The model learns the structure from the data itself.\n\nHere is how we change the workflow to building a random forest model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrf_model <- train(\n  SalePrice ~ .,        \n  data = train_data,\n  method = \"rf\",        \n  trControl = trainControl(\"cv\", number = 5),\n  tuneLength = 5\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict(rf_model, newdata = test_data) \nrf_result <- RMSE(pred, test_data$SalePrice)\nrf_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 234362\n```\n\n\n:::\n:::\n\n\n# Model Summary\n\nHere’s a summary of our model results using the same dataset and 5-fold cross-validation. We tried a simple linear model, a Lasso model, and a Random Forest model. Their RMSE values differ, with the more flexible, nonlinear Random Forest achieving the lowest RMSE.\n\nFlexible models can capture complex patterns better, but better performance isn’t guaranteed—it depends on the data’s shape and distribution. In machine learning, we usually test multiple models to find the one with the best prediction accuracy, which is at the core of this field.\n\n| Model Type               | RMSE                                                         |\n|----------------------------|--------------------------------------------|\n| Simple linear regression | 269,617    |\n| Lasso regression         | 268,423.9 |\n| Random Forest            | 234,362    |\n\nAs you’ve likely noticed, when we used different models, details like `tuneLength`, `preprocess` changed. The `caret` package supports over 230 machine learning algorithms. Each model comes with its own set of hyperparameters that need to be tuned. We’ll explore those in more detail next time.\n\n# Lab Report\n\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's also a popular dataset in the [Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview) community. You can find a wealth of prediction projects and explore the various approaches people have taken with this dataset.\n\nUsing this data, please fit three different models and run them to calculate their RMSE values. This will require some basic data preprocessing and proper model setup to ensure each model works correctly. Include a summary table that compares the RMSE results from the three models.\n\nIn your report, please present your workflow and reasoning, including how you prepared the data, selected models and variable, and addressed any data issues you might have encountered.\n\nPlease include all your work in a Quarto document and submit your Rendered HTML file to Canvas by the end of day, Tuesday, Sep 23.\n",
    "supporting": [
      "lab8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}