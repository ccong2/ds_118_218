[
  {
    "objectID": "modules/index3.html",
    "href": "modules/index3.html",
    "title": "Module 3 Communicate Data Science",
    "section": "",
    "text": "Transform static analyses into interactive tools that invite users to explore and discover insights on their own.\n\nInteractive Map and Dashboard\nWeb-app storytelling"
  },
  {
    "objectID": "modules/index1.html",
    "href": "modules/index1.html",
    "title": "Module 1 Descriptive Data Science",
    "section": "",
    "text": "We’ll explore the foundations of descriptive data science, learning how to summarize, visualize, and interpret datasets to uncover meaningful patterns.\n\nIntro to R\ndplyr for Data Wrangling\nggplot2 for Visualization\nEnhanced visualization using plotly and gt"
  },
  {
    "objectID": "labs/lab9.html",
    "href": "labs/lab9.html",
    "title": "Geospatial Machine Learning with ",
    "section": "",
    "text": "Last week, we saw how machine learning can be used for prediction, and that different models vary in their predictive accuracy. More flexible models, like random forests, tend to capture complex patterns better. This week, we’ll take a closer look at tree-based models, including decision trees, ensembles, and boosting methods. The spatial element is added and we will learn why generalizability across space is so important for geospatial machine learning.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(caret)\nlibrary(rattle)\nlibrary(gbm)"
  },
  {
    "objectID": "labs/lab9.html#how-was-the-forest-formed",
    "href": "labs/lab9.html#how-was-the-forest-formed",
    "title": "Geospatial Machine Learning with ",
    "section": "How was the forest formed?",
    "text": "How was the forest formed?\nRandom forest is built through two stages of random sampling. Each tree is trained on a random subset of the data, and at each split, each tree randomly selects a subset of predictors to consider. This added variation across trees mimics the wisdom of crowds: each tree sees a different part of the data and contributes its own “opinion,” leading to more stable and generalizable predictions.\n\nHow many trees are there in a forest\nIn the caret package, the number of trees in a random forest is controlled by the ntree parameter. You can pass it through the train() function. The default value is 500.\ntrain(\n  ...\n  ntree = 500\n)\n\n\nHow many predictors each tree uses\nmtry controls how many predictors are randomly selected and considered at each split in a tree. The random forest model in caret automatically tunes the mtry hyperparameter, meaning the model tests different values to see which one gives the best predictive performance.\nIn your model result below (rf_model), mtry shows the number of predictors randomly selected at each split of a tree. It tested 6 mtry values, because we have specified tuneLength = 6 in the train() function. After the 6 attempts, the the model says “The final value used for the model was mtry = 27.”\n\nrf_model\n\nRandom Forest \n\n1036 samples\n  14 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 828, 828, 829, 831, 828 \nResampling results across tuning parameters:\n\n  mtry  RMSE      Rsquared   MAE      \n   2    270273.4  0.7978104  150634.58\n   7    187092.3  0.8770591  106331.33\n  12    167880.8  0.8913759  100677.57\n  17    161211.3  0.8981539   99603.43\n  22    158843.8  0.8978341   99384.06\n  27    158736.9  0.8963383   99674.57\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 27.\n\n\nThe plot(rf_model) displays the model performance (e.g., RMSE) for each tested value of mtry.\n\nplot(rf_model)\n\n\n\n\n\n\n\n\n\n\nHyperparameter tuning\nNow we begin to see that machine learning models come with hyperparameters. Models learn patterns from data, they can behave differently depending on how they’re set up. A model can give you a random guess (underfitting)or try to accurately fit every single point (overfitting). What we want is a balance: a model that captures the real underlying patterns, but not the noise.\nHyperparameters are the “knobs” we use to tune that balance between accuracy and generalizability. Unlike parameters that are learned from the data (like coefficients in linear regression), hyperparameters are set by the users or selected through tuning (like cp in a decision tree, or mtry in a random forest).\nIf you’re not satisfied with the default values that the model tries, or if you want to test specific values (such 2, 4, 6, 8), you can manually tune them by specifying a tuning grid using the tuneGrid argument in the train() function:\n# Fit the model with your custom grid\nrf_model &lt;- train(\n  ...\n  tuneGrid = expand.grid(mtry = c(2, 4, 6, 8)),\n\n)"
  },
  {
    "objectID": "labs/lab9.html#two-ensemble-learning-methods",
    "href": "labs/lab9.html#two-ensemble-learning-methods",
    "title": "Geospatial Machine Learning with ",
    "section": "Two Ensemble Learning Methods",
    "text": "Two Ensemble Learning Methods\nBagging\nWe mentioned each decision tree is trained on a different subset of the training data. This process is called bagging, short for bootstrap aggregating. It works by randomly sampling the original training data with replacement, meaning the same observation can be selected multiple times.\nThe statistical term bootstrapping comes from the phrase “to pull oneself up by one’s bootstraps”, reflecting the idea that we generate many new datasets by resampling from the original data, without needing new data.\nBoosting\nWhile bagging builds many trees in parallel, boosting builds trees sequentially, where each new tree is trained to correct the mistakes made by the previous one. Both bagging and boosting are ensemble methods, they aggregate the outputs of multiple models, just in different ways\nIn boosting, the idea is to focus learning on the “hard” cases. The first tree might make many mistakes, and the second tree then tries to fix those mistakes by giving more weight to the misclassified or poorly predicted observations. This process continues, with each tree trained to minimize the residual error from the combined model so far."
  },
  {
    "objectID": "labs/lab9.html#gbm-hyperparameters",
    "href": "labs/lab9.html#gbm-hyperparameters",
    "title": "Geospatial Machine Learning with ",
    "section": "GBM Hyperparameters",
    "text": "GBM Hyperparameters\nThis table shows all the models available in train(), along with their tunable parameters. For example, you can look up rpart and see that its tuning parameter is cp, and for rf (random forest), it’s mtry.\nIf you scroll down to gbm (boosted tree model), you’ll notice it has four tuning parameters.\n\nWhat are they\n\nn.trees: the number of trees in the (sequenced) ensemble (typical number: 50-500)\ninteraction.depth: the maximum depth of each individual tree (usually start with 1-5)\nshrinkage: learning rate. how much each tree contributes to the final prediction. smaller value means slow learning. (Typical values: 0.01, 0.05, 0.1)\nn.minobsinnode: Minimum observations in terminal nodes. how small the leaf nodes (terminal nodes) can be. If this subpartition is trival, then stop it. Higher: less complicated trees. (Typical values: 10)\n\nWith more tuning knobs, it mean GBMs are prone to overfitting. It’s tempting to adjust to fit the training data very well. If it fits the training data too well, it loses generalbility\n\n\nHow to tune\n1.Let the model choose (automatic tuning): We mentioned we can set up tuneLength in the train function. In the previous example, we have set tuneLength = 3, the model automatically selected 3 values for each of the most influential parameter.\nIn our case, both n.trees (number of trees) and interaction.depth (tree depth) were varied, each with 3 values. This led to a total of 3 × 3 = 9 combinations, which you can see in the result table.\n\ngbm_model$results\n\n  shrinkage interaction.depth n.minobsinnode n.trees     RMSE  Rsquared\n1       0.1                 1             10      50 219364.0 0.8420349\n4       0.1                 2             10      50 202008.2 0.8568887\n7       0.1                 3             10      50 193100.0 0.8633882\n2       0.1                 1             10     100 207917.4 0.8522571\n5       0.1                 2             10     100 188575.1 0.8659353\n8       0.1                 3             10     100 186397.0 0.8691328\n3       0.1                 1             10     150 203157.6 0.8555506\n6       0.1                 2             10     150 182642.5 0.8719976\n9       0.1                 3             10     150 183749.3 0.8717811\n       MAE   RMSESD RsquaredSD    MAESD\n1 123631.8 21014.13 0.07218021 6894.726\n4 113605.3 18676.79 0.05863182 3698.699\n7 108567.7 22721.83 0.05961338 2748.247\n2 117687.1 22009.86 0.06230836 4904.312\n5 110150.1 15023.60 0.05295433 2653.846\n8 106545.5 19020.91 0.05349108 3606.791\n3 117843.7 17405.58 0.05745362 3946.568\n6 108353.7 16029.87 0.04942192 4009.147\n9 106603.3 14395.91 0.04824818 3646.324\n\n\n2.Manual tuning (full control): If you want more control, you can manually define the grid of values using tuneGrid = expand.grid(...) . The model will then try all possible combinations of the values you provide.\nIn the following code, I set up a grid where I will use a deeper tree (interaction.depth), and test it against 2 values of total number of trees (n.trees). This custom tuning improved our test RMSE.\n\ngbm_Grid &lt;-  expand.grid(n.trees = c(50, 100),  \n                         interaction.depth = 5,     \n                         shrinkage = 0.1,   \n                         n.minobsinnode = 10)\n\n\nset.seed(42)\ngbm_model_tuned &lt;- train(\n  SalePrice ~ .,        \n  data = train_data,\n  method = \"gbm\",        \n  trControl = trainControl(\"cv\", number = 5),\n  tuneGrid = gbm_Grid,\n  verbose = FALSE\n)\n\npred &lt;- predict(gbm_model_tuned, newdata = test_data) \ngbm_tuned_result &lt;- RMSE(pred, test_data$SalePrice)\ngbm_tuned_result\n\n[1] 140898.6\n\n\nAgain, we can see all the value combinations that the model used.\n\ngbm_model_tuned$results\n\n  shrinkage interaction.depth n.minobsinnode n.trees     RMSE  Rsquared\n1       0.1                 5             10      50 194408.7 0.8634318\n2       0.1                 5             10     100 186034.2 0.8668271\n       MAE   RMSESD RsquaredSD    MAESD\n1 109854.7 21113.45 0.05437980 4906.340\n2 107333.1 15643.44 0.04977026 4106.315\n\n\nHowever, trying too many combinations isn’t always necessary or helpful. Different models vary in how sensitive they are to tuning. Sometimes, it’s perfectly fine to stop at a solution that is “good enough”, especially when speed and simplicity matters."
  },
  {
    "objectID": "labs/lab9.html#gbm-feature-importance",
    "href": "labs/lab9.html#gbm-feature-importance",
    "title": "Geospatial Machine Learning with ",
    "section": "GBM Feature Importance",
    "text": "GBM Feature Importance\nWe can also plot the variable importance of the GBM model using varImp(). The spatial lag variable still ranks among the most important predictors.\n\ngbm_model_importance &lt;- varImp(gbm_model_tuned, scale = FALSE) \n\n# We can also visualize the most important features in our model this way...\nplot(gbm_model_importance, top = 5)"
  },
  {
    "objectID": "labs/lab7.html",
    "href": "labs/lab7.html",
    "title": "Build Shiny Apps with ",
    "section": "",
    "text": "In this lab, we’ll guide you through building a Shiny app to turn our Airbnb Leaflet map into an interactive version. It’s a gentle introduction to the Shiny app process, with plenty of potential for scaling up if you’re interested in exploring further in the future!\nA Shiny app is a plain and humble .R file, partly because it was developed long before Quarto documents and other tools. So, we will create the project folder for this week, then start a new R script file (File -&gt; New File -&gt; R script). Save the file with the name “app.R”."
  },
  {
    "objectID": "labs/lab7.html#preparation",
    "href": "labs/lab7.html#preparation",
    "title": "Build Shiny Apps with ",
    "section": "Preparation",
    "text": "Preparation\nLet’s continue using the city you worked on in your Lab 4 report, and build on your existing Leaflet code. Please put the “listings.csv” file that you downloaded for lab 4 report into your project folder. Then copy and paste the setup code below into your R script:\n\n# install.packages(\"shiny\")\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(tidyverse)\n\nairbnb &lt;- read_csv(\"listings.csv\")\n\nui &lt;- fluidPage(\n  # *Input functions go here\n  # *Output functions go here\n)\n\nserver &lt;- function(input, output) {\n  # output$id &lt;- render*function(input$id)\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "labs/lab7.html#design-your-ui",
    "href": "labs/lab7.html#design-your-ui",
    "title": "Build Shiny Apps with ",
    "section": "Design your UI",
    "text": "Design your UI\nFirst, we’ll walk through how to build a Shiny app like this for your city. In this example, there is one slider that allows users to control the price range, and the map will only display listings that fall within the selected price range.\nThe UI for this app consists of two main elements: the slider and the map. The slider is an input element, which means it receives user input, while the map is the output that displays the results.\nUI is built using input and output functions. The slider is linked to an input function, and the Leaflet map is linked to an output function. You can explore a complete list of input and output functions.\nFor this particular UI, I will first add a few lines of code to describe the slider’s appearance. The inputId is required, and you can give it any name that you can later use to reference the slider.\n\nui &lt;- fluidPage(\n  sliderInput(inputId = \"price_slider\",\n              label = \"Choose a price range\",\n              min = 1, max = 1500, value = c(500, 1000)),\n  # *Output functions\n)\n\nThe output is a Leaflet map, generated using the outputLeaflet() function. We will use this function here for the output object and assign it a name for reference.\n\nui &lt;- fluidPage(\n  sliderInput(inputId = \"price_slider\",\n              label = \"Choose a price range\",\n              min = 1, max = 1500, value = c(500, 1000)),\n  leafletOutput(outputId = \"map\")\n)\n\nWithout changing anything in the server part, let’s click “Run App” to see what we have now. A viewer will open up showing the layout of your app: a slider and a empty area ready for output map. Close the viewer. In order to actually show the map, we will need to process the data in the server function."
  },
  {
    "objectID": "labs/lab7.html#assemble-inputs-and-outputs-in-server",
    "href": "labs/lab7.html#assemble-inputs-and-outputs-in-server",
    "title": "Build Shiny Apps with ",
    "section": "Assemble inputs and outputs in server",
    "text": "Assemble inputs and outputs in server\nInside the curly braces of the server function template, we will write all the instructions to create the Leaflet map. It is recommended to compose the code in three steps:\nFirst, we will establish the connections between particular inputs and outputs. Right now we have only one input object price_slider, and one output object map, we can either write it down or mentally build this connection that the output$map will depend on the values of input$price_slider.\n\nserver &lt;- function(input, output) {\n  output$map &lt;- input$price_slider\n}\n\nNext, choose a render function that is associated with your output function. When building the UI we have reserved an area for leafletOutput(), so in the server we must use a renderLeaflet() function to produce the corresponding result. The connections between all output functions and render functions are also listed in the Shiny instruction page.\n\nserver &lt;- function(input, output) {\n  output$map &lt;- renderLeaflet(\n    input$price_slider\n  )\n}\n\nLastly, we’ll process the input variable. That means we will consider how user inputs - price ranges - influence the map. We are displaying only those within the selected price range by filtering out listings with prices outside the slider’s limits. In other words, we will use the slider’s two end values (input$price_slider[1] and input$price_slider[2]) to filter the airbnb dataset.\n\nserver &lt;- function(input, output) {\n  output$map &lt;- renderLeaflet({\n    # make a filtered dataset\n    filtered_airbnb &lt;- \n      airbnb |&gt; \n      filter(price &gt;= input$price_slider[1] & price &lt; input$price_slider[2])\n    \n    # pass the filtered dataset to a simple leaflet map\n    filtered_airbnb |&gt; \n      leaflet() |&gt; \n        addTiles() |&gt; \n        addCircleMarkers()\n  })\n}\n\nAll parentheses should be properly paired up. Note: We added a pairs of curly braces inside renderLeaflet(). This is necessary because {} allows us to group multiple lines of code into a single block that will be evaluated together.\nOnce you’ve completed the server code, click “Run App” again to check if the slider and reactive map appear as expected."
  },
  {
    "objectID": "labs/lab7.html#add-map-styles",
    "href": "labs/lab7.html#add-map-styles",
    "title": "Build Shiny Apps with ",
    "section": "Add map styles",
    "text": "Add map styles\nEverything you created for your previous Leaflet map will still work here! I’d suggest going back to your code from Exercise 4 and copying the leaflet() code to replace the basic version we used in the server. However, be mindful of two things:\n\nYou will need copy the color palette and the popup format too. If you have defined more elements for your map, make sure that they are all copied over. Place them inside the renderLeaflet() function.\n\n\nserver &lt;- function(input, output) {\n  output$map &lt;- renderLeaflet({\n    # make a filtered dataset\n    filtered_airbnb &lt;- \n      airbnb |&gt; \n      filter(price &gt;= input$price_slider[1] & price &lt; input$price_slider[2])\n    \n    popup_format &lt;-\n      paste0(\"&lt;b&gt;Name:&lt;/b&gt;\", filtered_airbnb$name, \"&lt;br&gt;\",\n             \"&lt;b&gt;Host Name: &lt;/b&gt;\", filtered_airbnb$host_name,\"&lt;br&gt;\",\n             \"&lt;b&gt;Price: &lt;/b&gt;\", \"$\", filtered_airbnb$price, \"&lt;br&gt;\"\n      )\n    \n    pal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\n    \n    # pass the filtered dataset to a simple leaflet map\n    filtered_airbnb |&gt; \n      leaflet() |&gt;\n      addProviderTiles(providers$CartoDB.Positron,\n                       group = \"CartoDB Positron\") |&gt;\n      addProviderTiles(\"Esri.WorldImagery\", \n                       group = \"ESRI World Imagery\") |&gt;\n      addProviderTiles(providers$CartoDB.DarkMatter, \n                       group = \"CartoDB Dark\") |&gt;\n      addCircleMarkers(data = filtered_airbnb,\n                       fillColor = ~pal(room_type),\n                       fillOpacity = 1,\n                       stroke = FALSE,\n                       radius = 2,\n                       popup = popup_format, \n                       group = \"Airbnb Listings\") |&gt;\n      addLegend(\n        position = 'topright',\n        pal = pal,\n        values = airbnb$room_type,\n        title = \"Room Type\") |&gt; \n      addLayersControl(\n        baseGroups = c(\"CartoDB Positron\", \n                       \"ESRI World Imagery\",\n                       \"CartoDB Dark\"),\n        overlayGroups = c(\"Airbnb Listings\", \"City Boundary\")\n      )\n  })\n}\n\nSome style elements might need adjustments for the filtered dataset. In our case, the popups use the filtered dataset because the data shown in the popups matches the markers in the filtered map view. We’ll update them to use filtered_airbnb .\n\nserver &lt;- function(input, output) {\n  output$map &lt;- renderLeaflet({\n    # make a filtered dataset\n    filtered_airbnb &lt;- \n      airbnb |&gt; \n      filter(price &gt;= input$price_slider[1] & price &lt; input$price_slider[2])\n    \n    popup_format &lt;-\n      paste0(\"&lt;b&gt;Name:&lt;/b&gt;\", filtered_airbnb$name, \"&lt;br&gt;\",\n             \"&lt;b&gt;Host Name: &lt;/b&gt;\", filtered_airbnb$host_name,\"&lt;br&gt;\",\n             \"&lt;b&gt;Price: &lt;/b&gt;\", \"$\", filtered_airbnb$price, \"&lt;br&gt;\"\n      )\n    \n    pal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\n    \n    # pass the filtered dataset to a simple leaflet map\n    filtered_airbnb |&gt; \n      leaflet() |&gt;\n        addProviderTiles(providers$CartoDB.Positron,\n                         group = \"CartoDB Positron\") |&gt;\n        addProviderTiles(\"Esri.WorldImagery\", \n                         group = \"ESRI World Imagery\") |&gt;\n        addProviderTiles(providers$CartoDB.DarkMatter, \n                         group = \"CartoDB Dark\") |&gt;\n        addCircleMarkers(data = filtered_airbnb,\n                         fillColor = ~pal(room_type),\n                         fillOpacity = 1,\n                         stroke = FALSE,\n                         radius = 2,\n                         popup = popup_format, \n                         group = \"Airbnb Listings\") |&gt;\n        addLegend(\n          position = 'topright',\n          pal = pal,\n          values = airbnb$room_type,\n          title = \"Room Type\") |&gt; \n        addLayersControl(\n          baseGroups = c(\"CartoDB Positron\", \n                         \"ESRI World Imagery\",\n                         \"CartoDB Dark\"),\n          overlayGroups = c(\"Airbnb Listings\", \"City Boundary\")\n        )\n  })\n}\n\nOnce you’ve finished editing the server code, click “Run App” and check if the price range works correctly, and if the colors and popups display as expected."
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Download and analyze Census data with ",
    "section": "",
    "text": "In this lab, we will use Decennial Census data and American Community Survey (ACS) data to examine long-term population trends in neighborhoods.\nThe Nathalie P. Voorhees Center has conducted research on neighborhood and community improvement in the City of Chicago. In their Three Cities report, they calculate the average per capita income for each census tract in Chicago for the years 1970, 1980, 1990, 2000, 2010, and 2017, and then compare it to the regional weighted average income. They have found Chicago has become more segregated by income over time and is losing its middle class.\n\nIn their study, census tracts are classified into “three cities”: those that have increased their share of regional income by 20% or more from 1970 to 2016, those that have changed by less than 20%, and those that have decreased by 20% or more. The study also summarizes socio-demographic characteristics in these areas.\n\nWe will take the idea of the Voorhees Center’s study to examine population change in Chicago. We’ll look at population by census tracts over decades to see if we can categorize tracts as gaining, losing, or staying stable in population.\nHere are the packages we are going to use today.\n\n# You may need to install the packages: tidycensus, tigris and gt\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(gt)"
  },
  {
    "objectID": "labs/lab5.html#review-the-process-of-making-census-api-calls",
    "href": "labs/lab5.html#review-the-process-of-making-census-api-calls",
    "title": "Download and analyze Census data with ",
    "section": "Review the Process of Making Census API Calls",
    "text": "Review the Process of Making Census API Calls\nWe will first review how to use tidycensus to download data for the most recent year 2022. You’ll need a Census API key (Here is where you can request one from the census bureau). After you’ve signed up for an API key, remember to activate the key from the email you receive.\nDeclaring install = TRUE when calling census_api_key() will install the key for use in future R sessions, which may be convenient for many users.\ncensus_api_key(\"yourkeyinquotationmarks\", install = TRUE)\n(If you’re having trouble with your key, restarting R might fix it!)\nAs a review, to complete the API call for ACS data, you will typically need to specify:\n\nA desired geography to look up (i.e. data by what area)\nThe variable IDs we want to look up (we have the table codes, and the complete variable list for 2022 5-year ACS.\nThe state and county names to define your study region.\nThe census year you want data for.\nThe survey you’d like to use (such as ACS 1-, or 5-year data)\n\nThe following code returns the number of total population by tract in Cook County, Illinois (where Chicago is in) in 2022. The table I consult is B01001: Sex by Age.\n\nF_2022 &lt;- get_acs(\n  geography = \"tract\",\n  state = \"IL\",\n  county = \"Cook\",\n  variables = \"B01001_001\",\n  year = 2022)\n\nAs discussed in class, we can choose to download multiple variables, reshape the table, calculate percentages, and then add the geometry column for quick mapping.\n\noutput = \"wide\" downloads data is in wide format, where each variable (e.g., totpop, Male_25to29) is represented as a separate column, and each row represent one tract.\ngeometry = TRUE returns a simple features (sf) object with spatial information saved for each tract. The returned table will include a geometry variable that enables spatial analysis.\n\nFor example, display one or more age groups, such as the male and female population aged 25–29:\n\nB01001_Vars &lt;- c(totpop = \"B01001_001\",\n                 Male_25to29 = \"B01001_011\", # look up the variable IDs\n                 Female_25to29 = \"B01001_035\")\n\nF_2022 &lt;- get_acs(\n  geography = \"tract\",\n  state = \"IL\",\n  county = \"Cook\",\n  variables = B01001_Vars, \n  output = \"wide\",\n  geometry = TRUE,\n  year = 2022)\n\n\nMap 1\n\nF_2022 |&gt; \n  # remove all Margin-of-error columns\n  select(-ends_with(\"M\")) |&gt; \n  # Clean up the column names\n  rename_with(~ str_remove(., \"E$\")) |&gt; \n  # Prepare for a faceted map\n  pivot_longer(cols = c(Male_25to29, Female_25to29),\n               names_to = \"gender_group\", values_to = \"group_pop\") |&gt; \n  \n  ggplot() +\n    geom_sf(aes(fill = group_pop/totpop))+\n    facet_wrap(~ gender_group) +\n    labs(fill = \"Proportion of Total Population\")+\n    theme_void()"
  },
  {
    "objectID": "labs/lab5.html#decennial-data-get_decennial",
    "href": "labs/lab5.html#decennial-data-get_decennial",
    "title": "Download and analyze Census data with ",
    "section": "Decennial data (get_decennial)",
    "text": "Decennial data (get_decennial)\nIn addition to get_acs(), tidycensus also provides get_decennial() for retrieving data from the US Decennial Census for the years 2000, 2010, and 2020. The function uses similar arguments, as shown below:\n\nF_2010 &lt;- get_decennial(\n  geography = \"tract\", \n  state = \"IL\",\n  county = \"Cook\",\n  variables = \"P001001\",\n  geometry = TRUE,\n  year = 2010)\n\nSo far you might have seen our first problem: for both API calls, we don’t have the option for getting tract-level data for cities - because tracts are not nested within places. We will come back to it later to cut out tracts within Chicago’s spatial boundary. That’s why we’ve included the argument geometry = TRUE, which will be needed for our spatial processing.\nOur second problem is that these two datasets have different number of observations. F_2022 contains 1332 tracts, F_2010 contains 1319 tracts. Additionally, more than 40 GEOIDs have changed between the datasets:\n\nsetdiff(F_2022$GEOID, F_2010$GEOID)\n\n [1] \"17031320101\" \"17031804316\" \"17031809402\" \"17031380600\" \"17031824128\"\n [6] \"17031490200\" \"17031804315\" \"17031330103\" \"17031824126\" \"17031844700\"\n[11] \"17031829904\" \"17031844600\" \"17031630600\" \"17031829903\" \"17031828508\"\n[16] \"17031820204\" \"17031803613\" \"17031330101\" \"17031806006\" \"17031804204\"\n[21] \"17031460800\" \"17031806005\" \"17031803614\" \"17031828507\" \"17031824508\"\n[26] \"17031824127\" \"17031824125\" \"17031824124\" \"17031824129\" \"17031804313\"\n[31] \"17031820203\" \"17031803615\" \"17031804314\" \"17031320102\" \"17031809401\"\n[36] \"17031803616\" \"17031804513\" \"17031824509\" \"17031804514\" \"17031804312\"\n[41] \"17031804203\" \"17031330102\" \"17031804512\" \"17031612200\"\n\n\nThe U.S. Census Bureau adjust geographies every 10 years. Census geographies are based on population, and population change over time. Some tracts stay the same, but others may split as populations grow or merge when populations decline. The Census Bureau keeps geographic relationship files to document the relationships between geographies over time.\nAdditionally, we cannot continue to use get_decennial() for the 1990 and 1970 data for our analysis. The main reason is that the Census Bureau has removed the API endpoints for earlier years."
  },
  {
    "objectID": "labs/lab5.html#longitudinal-tract-database-ltdb-data-download",
    "href": "labs/lab5.html#longitudinal-tract-database-ltdb-data-download",
    "title": "Download and analyze Census data with ",
    "section": "Longitudinal Tract Database (LTDB) Data Download",
    "text": "Longitudinal Tract Database (LTDB) Data Download\nAnalysts often face the challenge of geographic inconsistencies when working with longitudinal data. To make it easier for researchers, Brown University’s Longitudinal Tract Database (LTDB) offers demographic estimates for decennial census years from 1970 to 2020 adjusted to match the 2010 census tract boundaries. This means the tract areas and IDs match the 2010 geography, with attribute values recalculated based on population share.\nLet’s get familiar with LTDB data and how to use them. Go to the data download page. You will need to enter your email address and certify that you will follow terms of use for the data.\nReview the data standard descriptions a bit to see what datasets are available. In our analysis, we will only use the Standard Full Data Files for population counts.\nIn “Download Standard Full Data Files”, select the year 1970, click the download button. Then do the same for 1990 and 2020. Save the three .csv files into your project folder.\nNow we can import these datasets using read_csv. Simultaneously, we’ll use filter() to extract only the portion for Cook County.\nF_1970 &lt;- read_csv(\"LTDB_Std_1970_fullcount.csv\")|&gt;\n  filter(county == \"Cook County\" & state == \"IL\")\n  \nF_1990 &lt;- \n  read_csv(\"LTDB_Std_1990_fullcount.csv\") |&gt;\n  filter(county == \"Cook County\" & state == \"IL\")\n  \nF_2020 &lt;- \n  read_csv(\"ltdb_std_2020_fullcount.csv\") |&gt;\n  filter(substr(TRTID2010, 1, 5) == \"17031\")\n  \n# We will not use the 2022 data for the rest of the lab, so feel free to remove it if you'd like:\nrm(F_2022)"
  },
  {
    "objectID": "labs/lab5.html#compare-population-changes",
    "href": "labs/lab5.html#compare-population-changes",
    "title": "Download and analyze Census data with ",
    "section": "Compare population changes",
    "text": "Compare population changes\nWe now have compiled population data in one table. Our next step is to calculate the percentage change in population for each tract and classify the tracts based on whether their population has significantly decreased, increased, or remained largely unchanged in the past ~50 years, following the Voohver Center’s approach.\n\nTry to insert your own code to achieve the following, before you read along to the next chunk:\n\nCalculate the population change from 1970 to 2020 by computing the percentage increase (POP2020 - POP1970) / POP1970. Use mutate() for this and create a new column called “change”.\nCreate another column called “status” to classify the changes. Label the changes less than 10% as “Low Change”. Label changes greater than 10% as “Growing”, and less than -10% as “Declining”.\nNote that we also have a small number of missing values (NAs) in change because some 2010 tracts did not exist in earlier years. To account for that I included a fourth category in status named “Uncategorized”.\n\nHow to populate values based on multiple conditions? The traditional and more straightforward approach is using the if_else() statement. Combining it with mutate, it looks something like this:\n# Only for showing, do not run #\npop_data |&gt; \n  mutate(status = ifelse(change &lt; -0.1, \"declining\",\n                ifelse(change &gt; 0.1, \"growing\",\n                ifelse(is.na(change), \"uncategorized\", \"low change\"))))\nHowever, dplyr offers a more concise solution. Check out case_when, which simplifies handling multiple conditions by eliminating the need for nested if else statements within multiple parentheses.\n\npop_data &lt;- pop_data |&gt; \n  mutate(change = (POP2020 - POP1970)/POP1970)|&gt; \n  mutate(status = case_when(\n    is.na(change) ~ \"Uncategorized\",\n    change &gt; 0.10 ~ \"Group 1 - Growing\",\n    change &lt; -0.10 ~ \"Group 2 - Declining\",\n    TRUE ~ \"Group 3 - Low Change\"\n  ))"
  },
  {
    "objectID": "labs/lab5.html#create-summary-tables",
    "href": "labs/lab5.html#create-summary-tables",
    "title": "Download and analyze Census data with ",
    "section": "Create summary tables",
    "text": "Create summary tables\nAs the dataset is now prepared, we can use group_by() and summarise() to find out how many tracts are in each category.\n\npop_data |&gt; \n  group_by(status) |&gt;    \n  summarise(num_tracts = n())\n\n# A tibble: 4 × 2\n  status               num_tracts\n  &lt;chr&gt;                     &lt;int&gt;\n1 Group 1 - Growing           422\n2 Group 2 - Declining         639\n3 Group 3 - Low Change        256\n4 Uncategorized                 2\n\n\nSpeaking of summary tables, gt() is another powerful tool for creating customizable and polished tables with ease. For instance, we can add a few more lines in the previous code to enhance this table by adding titles and coloring. There are also many other styling options available.\n\npop_data |&gt; \n  group_by(Status = status) |&gt; \n  summarise(`Number of Tracts` = n()) |&gt; \n  gt() |&gt; \n  tab_header(title = \"Change in Population, 1970-2020\",\n             subtitle = \"Cook County, IL\") |&gt; \n  tab_options(column_labels.background.color = 'dodgerblue4')\n\n\n\n\n\n\n\nChange in Population, 1970-2020\n\n\nCook County, IL\n\n\nStatus\nNumber of Tracts\n\n\n\n\nGroup 1 - Growing\n422\n\n\nGroup 2 - Declining\n639\n\n\nGroup 3 - Low Change\n256\n\n\nUncategorized\n2"
  },
  {
    "objectID": "labs/lab5.html#obtain-census-geographies",
    "href": "labs/lab5.html#obtain-census-geographies",
    "title": "Download and analyze Census data with ",
    "section": "Obtain census geographies",
    "text": "Obtain census geographies\ntigris fetches census geographies. You can think of it as a programmatic way of downloading TIGER/Line Shapefiles. It uses geographic entities as function names. For example, tracts() indicates that you want to download tract boundaries, and county() indicates you want to download county boundaries, and so on.\nThe function place() fetches the boundary for census-designated places. The city of Chicago is one of such places in Illinois.\n\noptions(tigris_use_cache=TRUE) # This is to allow tigris to use caching for downloaded data so that we don't need to fetch the same data again the next time you need it.\n\n# Download the boundary of Chicago. \nchi_bnd &lt;- \n  places(state = \"IL\") |&gt; \n  filter(NAME == \"Chicago\")\n\nRecall that we have included spatial geometry for census tracts in our F_2010 dataset. We can plot both F_2010 and the Chicago boundary together to see the overlay.\n\nggplot() +\n  geom_sf(data = F_2010)+ # boundary of Cook County\n  geom_sf(data = chi_bnd, color = \"blue\", \n          linewidth = 0.5, fill = NA) # boundary of Chicago\n\n\n\n\n\n\n\n\nOkay, now we have:\n\nF_2010: A spatial object containing geometry for all tracts in Cook County.\npop_data: A table with tract-level population change in Cook County, including our “growing, declining, low change” categorizations.\nchi_bnd: A spatial object of Chicago’s boundary.\n\nWe can first attach the spatial information of F_2010 back to pop_data:\n\nchi_pop_data &lt;- \n  # join geometry to our result table \n  left_join(pop_data, F_2010, by = \"GEOID\") |&gt; \n  # make sure it is a simple feature object\n  st_as_sf() \n\nAnd then take the intersection of the census tracts to fit within Chicago’s boundary."
  },
  {
    "objectID": "labs/lab5.html#write-a-function",
    "href": "labs/lab5.html#write-a-function",
    "title": "Download and analyze Census data with ",
    "section": "Write a function",
    "text": "Write a function\nWhen doing intersections on census data, we often face the challenge of estimating the portion of population within boundaries. One method we often use is estimating the population based on the ratio of the intersected area to the total area.\n\nIf you’re already familiar with this concept, we can now encapsulate the “proportional split” approach into a reusable function. Writing a function allows us to automate repetitive tasks, such as “calculating the area ratio of overlapping regions and comparing the intersected area to the total area every time when two shapefiles intersect”.\nWriting your own function can be a bit less intuitive, but it works like outlining the entire process using “placeholder” variables. After the process is built, you can run the function by providing actual variables as inputs to replace those placeholders.\n\n# Define a function to calculate area ratio of overlapping areas\n# Perform intersection between a \"census tract\" and a \"boundary\"\n\ncalculate_area_ratio &lt;-\n  function(census_tracts, boundary) {\n    census_tracts &lt;-\n      census_tracts |&gt; mutate(total_area = st_area(geometry))\n    \n    intersected_tracts &lt;- st_intersection(census_tracts, boundary)\n    \n    intersected_tracts &lt;- intersected_tracts |&gt;\n      st_make_valid() |&gt;\n      mutate(\n        intersect_area = st_area(geometry),\n        area_ratio = as.numeric(intersect_area / total_area)\n      ) |&gt; \n      filter(as.numeric(intersect_area) &gt; 0)\n    return(intersected_tracts)\n  }\n\nThen, we can run the function we have defined, calculate_area_ratio() , on our two spatial object:\n\n# First prepare them with the correct CRS\n# Chicago uses State Plane Illinois East (ftUS), EPSG:3435\n# https://spatialreference.org/ref/epsg/3435/\nchi_pop_data &lt;- chi_pop_data |&gt; st_transform(3435)\nchi_bnd &lt;- chi_bnd |&gt; st_transform(3435)\n\n# Next input our two spatial objects into the function\nchi_pop_data &lt;-  \n  calculate_area_ratio(census_tracts = chi_pop_data, \n                       boundary = chi_bnd)\n\nAfter running the function, we will have a intersected spatial object that contains an area_ratio column, which is used for calculating population share.\n\nchi_pop_data &lt;- \n  chi_pop_data |&gt;  \n  mutate(POP1970 = POP1970 * area_ratio,\n         POP1990 = POP1990 * area_ratio,\n         POP2010 = POP2010 * area_ratio,\n         POP2020 = POP2020 * area_ratio) |&gt; \n  select(GEOID:status)\n\nTake a look at your chi_pop_data, if everything goes well, we will have a smaller dataset (869 rows) that looks like this."
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Data Visualization with ",
    "section": "",
    "text": "Airbnb Data\nInside Airbnb provides a freely available dataset that lets you explore Airbnb listings and related metrics for cities worldwide.\nOn this data downloading page, search for the City of Chicago. You’ll see a list of files with descriptions. Take a moment to review them. The first file, listings.csv.gz is “Detailed Listings data”. It contains raw information scraped directly from Airbnb’s website. Our task today is to extract meaningful insights from this dataset.\nIn the lecture, we briefly explored this data and you noted down some questions you want to investigate. Now, we’ll clean the data, create visualizations, and work toward answering those questions.\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(gt)\n\nThe .gz file is only a compressed file format. read_csv() automatically detects .gz and decompresses it on the fly.\nairbnb &lt;- read_csv(\"listings.csv.gz\")\nTake a look through the 79 variables included in the dataset along with the data dictionary. Try to understand what each one means and what it tells you about a property.\nStart a new Quarto file and begin by listing out three questions you’re curious about. These can be things like:\n\nHow does the price of an Airbnb listing vary by type of housing?\nAre there patterns in availability across neighborhoods?\nHow do different types of hosts set their minimum stay requirements?\n\nI’ll walk through a few examples to introduce some new packages and demonstrate how to work with different types of variables\n\n\nWorking with text data using stringr\nWe can come across with unstructured text input in our work. Examples include extracting all email addresses from text, counting how often a keyword appears, or cleaning up messy formatting (e.g., removing symbols like $ or *). These tasks involve string (i.e. characters) manipulation, and in R, we commonly use stringr, which is also part of tidyverse.\nTyping str_ will trigger autocomplete, allowing you to see all available functions in this package. For example:\nstr_c() combine multiple strings.\n\n# This combine street, city, state into full address\nstr_c(\"77 \", \"Massachusetts \", \"Ave\")\n\n[1] \"77 Massachusetts Ave\"\n\n\nstr_split() splits strings by a separator, which could be a space, a comma, etc.\n\n# This separates first and last name into two parts\nstr_split(string = \"Hadley Wickham\", pattern = \" \")\n\n[[1]]\n[1] \"Hadley\"  \"Wickham\"\n\n\nstr_pad() add padding characters to strings (either left or right)\n\n# This adds leading zeros to ZIP codes\nstr_pad(\"2140\", pad = \"0\", side = \"left\", width = 5)\n\n[1] \"02140\"\n\n\nHere we are going to use string modification to clean the price column in airbnb. If you look at the data (e.g., with glimpse(airbnb)), you’ll see that the prices are stored as characters, not numbers. Converting functions like as.numeric() won’t work directly here because the prices include a dollar sign ($) and commas, like this: “$1,200,000”. We first need to modify the strings to remove the dollar signs and commas, before we can convert them to numeric prices and calculate them.\nIn a more general sense, we will replace those symbols with an empty string (““), effectively removing them.\nstr_replace replaces by pattern.\n\nstr_replace(string = \"$1,200\", pattern = \",\", replacement = \"\") \n\n[1] \"$1200\"\n\n\nOr if use str_replace_all, it can detect and remove multiple symbols simutaneously, by wrapping them in brackets\n\nstr_replace_all(string = \"$1,200.00\", pattern =  \"[$,]\", replacement = \"\")\n\n[1] \"1200.00\"\n\n\nHere we make a small dataset that only include property id, room types and price:\n\ndf_price &lt;- airbnb |&gt; \n  select(id, room_type, price) |&gt; \n  mutate(price = str_replace_all(string = price, \n                                 pattern = \"[$,]\", \n                                 replacement = \"\")) |&gt; \n  mutate(price = as.numeric(price))\n\nThen we can create a bar chart that displays the median price for each type of property.\n\ndf_price |&gt; \n  filter(price &lt; 10000) |&gt; \n  group_by(room_type) |&gt; \n  summarise(median_price = median(price, na.rm = TRUE)) |&gt; \n  ggplot() + \n    geom_col(aes(x = room_type, y = median_price), \n             fill = \"#62A0CA\", width = 0.5) + \n  theme_bw()+\n  labs(x = \"Room Type\", y = \"Median Price\")\n\n\n\n\n\n\n\n\n\n\nInteractive graphs with plotly\nplotly is much larger than just the R universe, with implementations in Python, Julia, Javascript, and others. However, its interface with ggplot2 is practically easy to use! All we need to do is to use the ggplotly() function to convert a ggplot object into an interactive plotly plot.\n\ng &lt;- df_price |&gt; \n  filter(price &lt; 10000) |&gt; \n  group_by(room_type) |&gt; \n  summarise(median_price = median(price, na.rm = TRUE)) |&gt; \n  ggplot() + \n    geom_col(aes(x = room_type, y = median_price), \n             fill = \"#62A0CA\", width = 0.5) + \n  theme_bw()+\n  labs(x = \"Room Type\", y = \"Median Price\")\n\n\nggplotly(g)\n\n\n\n\n\n\n\nWorking with dates using lubridate\nSometimes, you’ll need to work with dates in your analysis. Date can come with different format, like “Sep 21, 2025”, “09/21/2025”, or “2025-09-21”, etc. One noteworthy lubridate function is to standardize it, and you can even extract the componets for\nymd (and its siblings mdy , dmy, and ymd_hms…). It’s a convenient function that converts strings into proper date objects. To use it, identify the order in which year, month, and day appear in your dates, then arrange “y”, “m”, and “d” in the same order.\n\ndf_age &lt;- airbnb |&gt; \n  select(id, host_since) |&gt; \n  mutate(host_since = ymd(host_since)) |&gt; \n  mutate(host_since_year = year(host_since)) \n\nThe above code shows that we can create another smaller dataset that includes the listing ID and the date the property was first listed (host_since). We then convert the character format of the date into a proper date using ymd(), and finally extract the year from it for further analysis. So that you can see how many properties started in each year.\nWe can visualize this result using a plotly treemap. A treemap shows proportions using nested rectangles, and it’s great for quickly comparing categories.\n\ndf_age |&gt; \n  count(host_since_year) |&gt; \n  plot_ly(\n    type = \"treemap\",\n    labels = ~host_since_year,\n    parents = \"\",  # No hierarchy, just one level\n    values = ~n\n  )\n\n\n\n\n\nPlotly uses its own plotting grammar, but it shares a lot of similarities with ggplot2. For example, you still map variables to aesthetics like labels, values, and hierarchy levels, and the structure of the plot is layered.\nI’d perfer to use ggplotly to convert almost any ggplot graphs to save me from having to remember a new set of functions. But for your interest in digging deeper into plotly, check out plotly R graph library and its ggplot2 integration examples.\n\n\nCreate Nicer Tables Using gt\nNow suppose you’re planning a trip to Chicago and want to stay in the “Logan Square” neighborhood. I decide to dig into the Airbnb dataset to find hosts who have both good ratings and are actively getting reviews.\nIn the following code, I start by consulting the data dictionary to identify relevant variables. Then I calculate how many reviews each host has received in the last 30 days, as well as their average ratings.\n\ndf_review &lt;- airbnb |&gt; \n  select(id, host_id, host_name, neighbourhood_cleansed,\n         number_of_reviews_l30d, review_scores_rating) \n\ntop_reviewed_hosts &lt;- df_review |&gt; \n  filter(neighbourhood_cleansed == \"Logan Square\") |&gt; \n  group_by(host_id, host_name, review_scores_rating) |&gt; \n  summarise(n = sum(number_of_reviews_l30d),.groups = \"drop\") |&gt; \n  arrange(desc(n)) |&gt; \n  head(10)\n\n\ntop_reviewed_hosts\n\n# A tibble: 10 × 4\n     host_id host_name  review_scores_rating     n\n       &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;\n 1  24129606 Leo & Alex                 4.84    14\n 2 526970195 Jevaux                     4.89    14\n 3 245386525 Hanna                      5       10\n 4  74207440 Daniel                     4.75     8\n 5 245386525 Hanna                      4.92     8\n 6   3069675 Gabrielle                  4.98     7\n 7  17160709 Cindy                      4.91     7\n 8  37420568 Zoë                        4.9      7\n 9 211206098 Ramon                      4.91     7\n10 349642200 Lourdes                    4.97     7\n\n\ngt can create more presentable, and customizable tables. For instance, we can add a few more lines to enhance this table by adding colorization based on these values.\n\ntop_reviewed_hosts &lt;- \n  top_reviewed_hosts |&gt; \n  rename(\n    `Host ID` = host_id,\n    `Host Name` = host_name,\n    `# Reviews` = n,\n    `Rating` = review_scores_rating\n  ) \n\ntop_reviewed_hosts|&gt; \n  gt() |&gt; \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"#F0F0F0\")  # Optional: light gray background\n    ),\n    locations = cells_column_labels(everything()))\n\n\n\n\n\n\n\nHost ID\nHost Name\nRating\n# Reviews\n\n\n\n\n24129606\nLeo & Alex\n4.84\n14\n\n\n526970195\nJevaux\n4.89\n14\n\n\n245386525\nHanna\n5.00\n10\n\n\n74207440\nDaniel\n4.75\n8\n\n\n245386525\nHanna\n4.92\n8\n\n\n3069675\nGabrielle\n4.98\n7\n\n\n17160709\nCindy\n4.91\n7\n\n\n37420568\nZoë\n4.90\n7\n\n\n211206098\nRamon\n4.91\n7\n\n\n349642200\nLourdes\n4.97\n7\n\n\n\n\n\n\n\n\ntop_reviewed_hosts|&gt; \n  gt() |&gt; \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"#F0F0F0\")  # Optional: light gray background\n    ),\n    locations = cells_column_labels(everything())) |&gt; \n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"#F0F0F0\")  # Optional: light gray background\n    ),\n    locations = cells_column_labels(everything())) |&gt;\n  tab_header(\n    title = \"Top Reviewed Airbnb Hosts in the last 30 Days\",\n    subtitle = \"Logan Square, Chicago\") |&gt;\n  data_color(\n    columns = Rating,\n    colors = scales::col_numeric(\n    palette = c(\"white\", \"red\"),\n    domain  = range(top_reviewed_hosts$Rating)\n    )) |&gt; \n  data_color(\n    columns = `# Reviews`,\n    colors = scales::col_numeric(\n    palette = c(\"white\", \"red\"),\n    domain  = range(top_reviewed_hosts$`# Reviews`)\n    )\n  )\n\n\n\n\n\n\n\nTop Reviewed Airbnb Hosts in the last 30 Days\n\n\nLogan Square, Chicago\n\n\nHost ID\nHost Name\nRating\n# Reviews\n\n\n\n\n24129606\nLeo & Alex\n4.84\n14\n\n\n526970195\nJevaux\n4.89\n14\n\n\n245386525\nHanna\n5.00\n10\n\n\n74207440\nDaniel\n4.75\n8\n\n\n245386525\nHanna\n4.92\n8\n\n\n3069675\nGabrielle\n4.98\n7\n\n\n17160709\nCindy\n4.91\n7\n\n\n37420568\nZoë\n4.90\n7\n\n\n211206098\nRamon\n4.91\n7\n\n\n349642200\nLourdes\n4.97\n7\n\n\n\n\n\n\n\n\n\nCombine Datasets\nWhen we explore the dataset, we are likely working a few variable combinations nad might have created some smaller dataset for convenience, then maybe we we want to bring different pieces together, for example, to look at the price of properties by the year they were listed.\nIf the smaller datasets you’re combining have the same number of rows and you haven’t changed the row order or removed any entries (like with our df_price and df_age), simply combining them works.\n\ndf &lt;- bind_cols(df_price, df_age)\n\nBut a more robust and flexible way to keep an identifiable variable (such as id ) in both datasets, so that we can combine them by using left_join(). This function merges two datasets by a shared key column and keeps all rows from the left dataset, adding matching rows from the right. If there are entries in the left dataset with no match in the right, those values will be filled with NA, signaling no match was found.\n\ndf &lt;- left_join(df_price, df_age, by = \"id\")\n\n\n\nLab Report\nBuilding on what we’ve learned in our data wrangling and visualization, your task this week is to explore the Airbnb dataset in a city that interests you.\nAt the beginning of your Quarto document, you have listed three questions you’re curious about. Start by selecting one city from Inside Airbnb. Using your questions as the guide, clean the data and create smaller, focused datasets in whatever way makes the most sense for your analysis. Then, create visualizations to help answer each of your three questions.\nChoose visualization types that you feel are most effective for communicating your insights. You’re welcome to use and refine ggplot() for your plots. However, to apply what we practiced today, your visualizations should include at least one interactive plotly graph and one table created by gt.\nIn your write-up, reflect on what your answers to these questions tell you about the Airbnb market in your chosen city. What patterns do you notice? What surprises you?\nPlease complete your Quarto document and upload your rendered HTML file to Canvas by the end of day, Tuesday, Sep 23."
  },
  {
    "objectID": "labs/lab10.html",
    "href": "labs/lab10.html",
    "title": "Fairness-Aware Machine Learning with ",
    "section": "",
    "text": "library(tidyverse)\nlibrary(caret)\nlibrary(mlr3fairness)"
  },
  {
    "objectID": "labs/lab10.html#model-accuracy",
    "href": "labs/lab10.html#model-accuracy",
    "title": "Fairness-Aware Machine Learning with ",
    "section": "Model Accuracy",
    "text": "Model Accuracy\nHow well does COMPAS predict reoffending? One way to measure this is by checking the model’s accuracy. Since this is a classification problem with “Yes” or “No” outcomes, accuracy is defined as the proportion of correct predictions: both when the model predicts reoffending and the person does reoffend, and when it predicts no reoffending and the person does not.\nIn other words, accuracy tells us how often the model is correct overall.\n\ncorrect_predictions &lt;- compas |&gt;\n  filter(\n    (predicted == \"yes\" & actual == \"yes\") |\n    (predicted == \"no\" & actual == \"no\")\n)\n\naccuracy &lt;- nrow(correct_predictions) / nrow(compas)\naccuracy\n\n[1] 0.6607259\n\n\nIn our case, the accuracy of the COMPAS model is about 0.66, meaning it makes the correct prediction 66% of the time. It’s better than random guessing (50%), although only moderately."
  },
  {
    "objectID": "labs/lab10.html#confusion-matrix",
    "href": "labs/lab10.html#confusion-matrix",
    "title": "Fairness-Aware Machine Learning with ",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nAccuracy is not the best metrics for evaluating classification models, as it doesn’t show how it makes mistakes. It treats a false positive (predicting someone will reoffend when they won’t) the same as a false negative (predicting someone won’t reoffend when they will). In practice, these errors can have very different consequences.\nConfusion matrix breaks down the types of correct and incorrect predictions:\n\n# True Positive: predicted recidivism, actually recidivated\nTP &lt;- sum(compas$predicted == \"yes\" & compas$actual == \"yes\")  \n\n# True Negative: predicted no recidivism, didn't recidivate\nTN &lt;- sum(compas$predicted == \"no\" & compas$actual == \"no\")  \n\n# False Positive: predicted recidivism, didn't recidivate\nFP &lt;- sum(compas$predicted == \"yes\" & compas$actual == \"no\")  \n\n# False Negative: predicted no recidivism, actually recidivated\nFN &lt;- sum(compas$predicted == \"no\" & compas$actual == \"yes\")  \n\n# Create a matrix manually\nconf_matrix &lt;- matrix(c(TN, FN, FP, TP),\n                      nrow = 2,\n                      dimnames = list(Prediction = c(\"N\", \"P\"),\n                                      Reference = c(\"N\", \"P\")))\nconf_matrix\n\n          Reference\nPrediction    N    P\n         N 2345 1018\n         P 1076 1733\n\n\nThis matrix reads:\n           Reference (Actual)\nPrediction     0      1\n         -----------------\n      0 |   TN     FN\n      1 |   FP     TP\nRecall that a perfect classifier would only have non-zero values on it’s main diagonal (TN and TP).\nTo put this result in our context, among the 3,421 (first column) people who actually did not reoffend, 1,076 were misclassified. That’s about 31.5%, or nearly one-third of non-reoffenders were flagged as high risk. This is called the False Positive Rate (FPR): the proportion of non-reoffenders incorrectly predicted as reoffenders."
  },
  {
    "objectID": "labs/lab10.html#breaking-down-by-racial-groups",
    "href": "labs/lab10.html#breaking-down-by-racial-groups",
    "title": "Fairness-Aware Machine Learning with ",
    "section": "Breaking down by racial groups",
    "text": "Breaking down by racial groups\nIf we take a closer look and break down the confusion matrix by racial groups, we will see that the model’s errors are not evenly distributed.\nThe analysis by Propublica has this quote:\n\n“Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts”\n\nThis means the FPR—predicting someone will reoffend when they won’t—is significantly higher for Black defendants compared to white defendants.\nWe’ll now reproduce this analysis ourselves.\nTo make this easier and reusable, we’ll define a function that generates a confusion matrix for any filtered dataset.\n\n# Define a function to calculate confusion matrix cells by race\nconfusion_by_race &lt;- function(df, race_name) {\n  df_race &lt;- df %&gt;% filter(race == race_name)\n  \n  TP &lt;- sum(df_race$predicted == \"yes\" & df_race$actual == \"yes\")\n  TN &lt;- sum(df_race$predicted == \"no\" & df_race$actual == \"no\")\n  FP &lt;- sum(df_race$predicted == \"yes\" & df_race$actual == \"no\")\n  FN &lt;- sum(df_race$predicted == \"no\" & df_race$actual == \"yes\")\n  \n  return(c(TP = TP, TN = TN, FP = FP, FN = FN))\n}\n\nThen calculate the confusion matrix for Black and White population, separately.\n\n# Remove other objects from the workspace to start fresh\nrm(list = setdiff(ls(), c(\"compas\", \"confusion_by_race\")))\n\n# Calculate for Black affenders\nb_vals &lt;- confusion_by_race(compas, \"African-American\")\nb_fpr &lt;- b_vals[\"FP\"]/(b_vals[\"FP\"]+b_vals[\"TN\"])\n\n# Calculate for White affenders\nw_vals &lt;- confusion_by_race(compas, \"Caucasian\")\nw_fpr &lt;- w_vals[\"FP\"]/(w_vals[\"FP\"]+ w_vals[\"TN\"])\n\n# Print our conclusions\nstr_glue(\n  \"Black defendants who did not recidivate over a two-year period \",\n  \"were nearly twice as likely to be misclassified as higher risk \",\n  \"compared to their white counterparts ({round(b_fpr*100)}% vs. {round(w_fpr*100)}%).\")\n\nBlack defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (42% vs. 22%).\n\n\nPropublica has another quote:\n\n“White defendants were often predicted to be less risky than they were. Our analysis found that white defendants who re-offended within the next two years were mistakenly labeled low risk almost twice as often as black re-offenders”\n\nThis points to another type of model error: False Negative Rate (FNR). It tells us, among all people who actually reoffended (the actual positives), how many were missed by the model (predicted as low risk).\nGoing back to our matrix, FNR is saying, out of all the actual reoffenders (second column), how many were incorrectly predicted as “no” (FN)\n           Reference (Actual)\nPrediction     0      1\n         -----------------\n      0 |   TN     FN\n      1 |   FP     TP\n\n# Calculate for Black reaffenders\nb_fnr &lt;- b_vals[\"FN\"]/(b_vals[\"FN\"]+b_vals[\"TP\"]) \n\n# Calculate for White reaffenders\nw_fnr &lt;- w_vals[\"FN\"]/(w_vals[\"FN\"]+ w_vals[\"TP\"])\n\nstr_glue(\n  \"White defendants who reoffend were given more lenient risk scores. \\n\",\n  \"{round(w_fnr*100)}% of White defendants were wrongly labeled as low risk compared with {round(b_fnr*100)}% for Black defendants.\")\n\nWhite defendants who reoffend were given more lenient risk scores. \n50% of White defendants were wrongly labeled as low risk compared with 28% for Black defendants.\n\n\nSteif’s book refers to this kind of pattern as disparate impact: when a decision-making process seems neutral on the surface but still results in unequal outcomes for certain groups. In this case, the COMPAS model tends to over-predict risk for Black defendants (higher False Positive Rate) and under-predict for White defendants (higher False Negative Rate), leading to unequal treatment, even though race is not explicitly used as a factor.\nDoes this mean the COMPAS model simply wasn’t effective? To explore this further, let’s try building our own prediction model using logistic regression and see how well it performs."
  },
  {
    "objectID": "labs/lab10.html#variable-selection",
    "href": "labs/lab10.html#variable-selection",
    "title": "Fairness-Aware Machine Learning with ",
    "section": "Variable Selection",
    "text": "Variable Selection\nChoosing which variables to include in a regression model is a critical challenge in algorithmic decision-making. Steif’s book highlights an important tension around using features like race or prior_count.\nIf we include race in a multiple linear regression (MLR), the results table will show the effect of the race variable, which might read: “Being race X increases predicted recidivism by 0.2”. This kind of output is problematic. It means the model will consistently add 0.2 for defendants of race X. In other words, it encodes bias into the predictions.\nEven if you remove race as a predictor, the model may still “learn” patterns related to race, because things like priors_count are correlated with race due to historical and systemic biases in policing. Similarly, things such as redlining neighborhoods and credit scores, which are closely connected to race or income due to historical segregation and unequal access to resources, can lead algorithms to systematically make worse predictions for people from certain socioeconomic groups.\nWhat we can do to address this issue is broadly referred to as Fairness-Aware Modeling. This includes a range of strategies: understanding the data deeply, being cautious about variables that may act as proxies for protected characteristics, and applying fairness techniques such as adjusting decision thresholds. We’ll try to build and improve a logistic regression model using some of these ideas."
  },
  {
    "objectID": "labs/lab10.html#model-building",
    "href": "labs/lab10.html#model-building",
    "title": "Fairness-Aware Machine Learning with ",
    "section": "Model Building",
    "text": "Model Building\nWe’ll now repeat the modeling workflow you’ve used in the past two classes. We’ll exclude race from the predictors, but include priors_count, while acknowledging that it may act as a problematic proxy for race.\n\nmodeldata &lt;- compas \n\nset.seed(42)\ntrain_index &lt;- createDataPartition(\n  y = modeldata$two_year_recid,\n  p = 0.7,\n  list = FALSE\n)\n\ntrain_data &lt;- modeldata[train_index, ]\ntest_data  &lt;- modeldata[-train_index, ]\n\nIn the train() function, we set method = \"glm\" to specify that we’re using a generalized linear model. family = \"binomial\" indicates that the outcome variable is binary. The trainControl() function is set up as before to perform five-fold cross-validation.\n\nset.seed(42)\nlogit_model &lt;- train(\n  actual ~ age + sex + c_charge_degree + priors_count + length_of_stay, \n  data = train_data,\n  method = \"glm\",        \n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 5)\n)"
  },
  {
    "objectID": "labs/lab10.html#our-result",
    "href": "labs/lab10.html#our-result",
    "title": "Fairness-Aware Machine Learning with ",
    "section": "Our Result",
    "text": "Our Result\nNow we use the trained model to predict the classes for the test dataset.\n\npred_class &lt;- predict(logit_model, newdata = test_data)\n\nAfter generating these predicted classes, we attach them back to the test dataset for the subsequent confusion matrix calculation.\n\n# attach to the test data\ntest_pred_class &lt;- test_data %&gt;%\n  mutate(predicted = pred_class)\n\nReuse our previous function confusion_by_race to calculate FPR by race:\n\nb_vals &lt;- confusion_by_race(test_pred_class, \"African-American\")\nb_fpr &lt;- b_vals[\"FP\"]/(b_vals[\"FP\"]+b_vals[\"TN\"])\n\nw_vals &lt;- confusion_by_race(test_pred_class, \"Caucasian\")\nw_fpr &lt;- w_vals[\"FP\"]/(w_vals[\"FP\"]+ w_vals[\"TN\"])\n\nstr_glue(\n  \"Black defendants who did not recidivate but misclassified as higher risk: {round(b_fpr*100)}%. \\n\",\n  \"White defendants who did not recidivate but misclassified as higher risk: {round(w_fpr*100)}%).\")\n\nBlack defendants who did not recidivate but misclassified as higher risk: 34%. \nWhite defendants who did not recidivate but misclassified as higher risk: 16%).\n\n\nAlthough our model has lower errors rate than COMPAS, it produces very similar results and shows the same kind of disparities that are roughly twice as large."
  },
  {
    "objectID": "labs/lab10.html#threshold-tuning",
    "href": "labs/lab10.html#threshold-tuning",
    "title": "Fairness-Aware Machine Learning with ",
    "section": "Threshold Tuning",
    "text": "Threshold Tuning\nWe can’t undo the historical patterns that shaped the data, and we often can’t completely avoid using variables that carry implicit bias, especially they carry predictive power. So what can we do?\nIn logistic regression, the model doesn’t directly predict a “yes” or “no” outcome. It predicts a probability between 0 and 1 that an observation belongs to the positive class. A final classification decision still needs to be made. By default, the threshold is set at 0.5, meaning if the predicted probability is greater than 0.5, the case is classified as “yes” (positive), otherwise “no” (negative).\nBy setting type = \"prob\" in the following code, we get predicted probabilities instead of class labels. This lets us adjust the decision threshold. For example, classifying a case as “yes” only if the probability is above 0.7, rather than the default 0.5. Changing the threshold affects how predictions fall into true or false positives and negatives, which in turn impacts error rates and fairness.\n\n# This is the probability result\npred_probs &lt;- predict(logit_model, newdata = test_data, type = \"prob\")\n\nNow let’s set different classification thresholds for different groups!\n\n# Attach pred_probs back to test data\ntest_pred_prob &lt;- test_data %&gt;%\n  mutate(pred_prob = pred_probs$yes)\n\n# Define threshold we want to use\nthresholds &lt;- list(\n  African_American = 0.7,\n  Caucasian = 0.6\n)\n\n# Create a new \"predicted\" column based on group-specific thresholds.\n# For African-American and Caucasian individuals, apply separate thresholds.\n# For all others, use the default threshold of 0.5.\ntest_pred_prob &lt;- test_pred_prob %&gt;%\n  mutate(predicted = case_when(\n    race == \"African-American\" & pred_prob &gt; thresholds$African_American ~ \"yes\",\n    race == \"African-American\" & pred_prob &lt;= thresholds$African_American ~ \"no\",\n    race == \"Caucasian\" & pred_prob &gt; thresholds$Caucasian ~ \"yes\",\n    race == \"Caucasian\" & pred_prob &lt;= thresholds$Caucasian ~ \"no\",\n    pred_prob &gt; 0.5 ~ \"yes\",\n    TRUE ~ \"no\"\n  ))\n\n# three metrics, FNR, FPR and Accuracy for both race\n\nNow that we’ve applied group-specific thresholds, we calculate and compare performance metrics: False Positive Rate (FPR), False Negative Rate (FNR), and Accuracy, for African-American and Caucasian defendants.\n\nb_vals &lt;- confusion_by_race(test_pred_prob, \"African-American\")\nb_fpr &lt;- b_vals[\"FP\"] / (b_vals[\"FP\"] + b_vals[\"TN\"])\nb_fnr &lt;- b_vals[\"FN\"] / (b_vals[\"FN\"] + b_vals[\"TP\"])\nb_acc &lt;- (b_vals[\"TP\"] + b_vals[\"TN\"]) / sum(b_vals)\n\nw_vals &lt;- confusion_by_race(test_pred_prob, \"Caucasian\")\nw_fpr &lt;- w_vals[\"FP\"] / (w_vals[\"FP\"] + w_vals[\"TN\"])\nw_fnr &lt;- w_vals[\"FN\"] / (w_vals[\"FN\"] + w_vals[\"TP\"])\nw_acc &lt;- (w_vals[\"TP\"] + w_vals[\"TN\"]) / sum(w_vals)\n\n# Print results\nstr_glue(\n  \"Black Defendants:\\n\",\n  \"  FPR: {round(b_fpr * 100, 1)}%\\n\",\n  \"  FNR: {round(b_fnr * 100, 1)}%\\n\",\n  \"  Accuracy: {round(b_acc * 100, 1)}%\\n\"\n)\n\nBlack Defendants:\nFPR: 9.8%\nFNR: 72.5%\nAccuracy: 56.8%\n\nstr_glue(\n  \"White Defendants:\\n\",\n  \"  FPR: {round(w_fpr * 100, 1)}%\\n\",\n  \"  FNR: {round(w_fnr * 100, 1)}%\\n\",\n  \"  Accuracy: {round(w_acc * 100, 1)}%\\n\"\n)\n\nWhite Defendants:\nFPR: 5%\nFNR: 80.2%\nAccuracy: 65.6%\n\n\nSteif’s book introduced a few guildlines for an “equity threshold”,\n\nMinimize differences in False Positive Rates (FPR) and False Negative Rates (FNR) between racial groups, while keeping both rates relatively low.\nMinimize differences in Accuracy rates between racial groups, while keeping the overall Accuracy high.\n\nHow would you interpret our new results? Would you consider this a better and more equitable model, or not? As discussed in the book, expect trade-offs: no single threshold will perfectly balance accuracy and fairness across all groups, which reflects real-world complexity in social data."
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Lab Overview",
    "section": "",
    "text": "Here you’ll find links and resources for each lab session:\n\nLab 1: Data Wrangling: Cambridge Building Energy\nLab 2: Making Exploratory Graphs: Opportunity Zones\nLab 3: Enhancing Visualizations: Airbnb Data\nLab 4: Spatial Analysis: Neighborhood Built Environment\nLab 5: Census Data: Describe Population Change\nLab 6: Online Map and Dashboard\nLab 7: Shiny Apps\nLab 8: Build Machine Learning Models: Predict Housing Prices\nLab 9: Tree-Based Models and Parameter Tuning\nLab 10: Fairness-Aware ML Models: Predict Recidivism"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the 11.118/218",
    "section": "",
    "text": "Urban science draws on statistics, visualization, and spatial analysis techniques to gain deeper insights into cities and actively contribute to their development. In this course, we’ll dive into the dynamic world of urban science by learning how to tell stories about cities and neighborhoods, covering a range of topics including demographic analysis, health and transportation, and using R as our primary quantitative analysis and interactive visualization tool.\n\n\n  \n    \n      Module 1Descriptive Data Science\n    \n  \n\n  \n    \n      Module 2Geospatial Data Science\n    \n  \n\n  \n    \n      Module 3Communicate Data Science\n    \n  \n\n  \n    \n      Module 4Predictive Data Science"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Get started with ",
    "section": "",
    "text": "This exercise provides some more structured exploration and practice with Quarto Document. We will mix Markdown sections with code chunks, and experiment with working with a tabular dataset."
  },
  {
    "objectID": "labs/lab1.html#set-up-a-new-project",
    "href": "labs/lab1.html#set-up-a-new-project",
    "title": "Get started with ",
    "section": "Set up a New Project",
    "text": "Set up a New Project\nWe have talked about files paths in class, and the importance of setting a directory to work out of. Most of the time we can use setwd(), this works fine until when you want to move your working folder to another computer or share your work. If you write out a path in the code, it might not work on another computer if that directory does not exist.\nIn our labs, we are going to use R Projects to organize our works and make sure we don’t lose files. R projects organize all files related to a project in one place and setting up relative file paths. Let’s start by setting up a project for our exercise.\nLaunch RStudio, then click File - New Project… A dialog box will open up. Select New Directory, then New Project. Here, you can create a new folder to save everything related to this project. For example, I navigated to my D:/Fall25 folder and created a new folder there called Lab 1:\n\nClick the button “Create Project”. R will take a second to refresh. Then you will see in your Files tab that you have been directed to your current working directory: D:/Fall25/Lab 1. You will also see a .Rproj file in that folder.\n\nThe .Rproj file serves as a reference point that R uses to locate all files associated with the project. If you save all files related to Lab 1 in this folder, all relative paths remain intact and consistently applied.\nNote: In future sessions, I may provide you with a project folder containing data. As long as you launch RStudio by double-clicking the .Rproj file, you will be taken directly to the project’s home directory."
  },
  {
    "objectID": "labs/lab1.html#practice-formatting-text-with-quarto",
    "href": "labs/lab1.html#practice-formatting-text-with-quarto",
    "title": "Get started with ",
    "section": "Practice formatting text with Quarto",
    "text": "Practice formatting text with Quarto\nNow go to File - New File - Quarto Document to create a new Quarto document. The prompt shown below will appear. Type in a document title (e.g. Lab 1) and your name. Keep the radio button for HTML selected.\n\nYou will then see a template file. At the very top you will see the YAML (or “Yet Another Markdown Language”) header which begins and ends with three dashes ---. The YAML header determines how your document will be rendered to your desired output format. Now it specifies the title, author, output format and text editor.\nTo get an idea of how everything works, let’s click the “Render” button on top of your toolbar.\nWhen prompted, give this file a name, it will be saved in the folder where your “.Rproj” file is, as a .qmd file.\nYou will now see a formatted document in a web browser. Switch between your code and the document back and forth to see where each part of the code is placed in the rendered HTML file.\nNow we can add a new line to the YAML header:\n\ndate: &lt;insert the date that the file is created&gt;.\n\nRender it again and see where the difference is.\nThere can be other options specified in the YAML, particularly if you are rendering to a format other than HTML (such as pdf, or Word, see all formats).\nOn the very left of this toolbar, click the “Source” button to switch Markdown editing mode. These sections of text typically explain or provide context for the code and graphics, and they are formatted using Markdown syntax. For example:\n\n#: a header element.\n**: bold text.\n*: italic text.\n` : code blocks.\n\nOverall, the Visual interface looks pretty much like a Word document. There is a toolbar that allows you to make bold or italic text, create a bullet list, insert a link or an image, insert a code block, etc.\nNow let’s delete everything below the YAML header in the template file, so that we will start creating our own formatted report.\n\nExercise\nIn 2014, the City of Cambridge passed a local ordinance on building energy use disclosure. Spend a few moments reviewing this website to become familiar with the ordinance (in general). Then, add 2-3 sentences below your YAML section that explain the following:\n\nWhat does the Building Energy Use Disclosure Ordinance require?\nWhat kind of data have been compiled and where to find them?\n\nYou may edit your text either in the “Source” or “Visual” panel, or toggle between them to get familiar with both. Make sure to make gratuitous use of bold, italics, bullet points, etc. in your text.\nWhen you finish, save your file and click Render again. You can immediately see your nicely formatted document in a web browser."
  },
  {
    "objectID": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "href": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "title": "Get started with ",
    "section": "Select: selects a subset of columns.",
    "text": "Select: selects a subset of columns.\nIn the energy dataset, we probably don’t need all of the 67 columns. So we can make it a smaller dataset by specifying a few columns to keep.\ndataset |&gt; select(Column1, Column2)\nInsert a new code chunk in your document like this one below. We will only keep 9 columns for the following analysis. Let’s start by typing the code ourselves to see what shows up along the way. You can type the pipe |&gt; operator in using Shift+Ctrl/Cmd+M.\nenergy |&gt;\n  select(\n    `Data Year`,\n    `BEUDO Category`,\n    Owner,\n    `Year Built`,\n    `Primary Property Type - Self Selected`,\n    `Total GHG Emissions (Metric Tons CO2e)`,\n    `Total GHG Emissions Intensity (kgCO2e/ft2)`,\n    Longitude,\n    Latitude\n  ) \nSome of the column names are surrounded by backticks (`), that’s because they include special characters or spaces (such as spaces and () ). The use of backticks is a means of preserving these unique naming attributes. If you just keep typing the column names, dplyr will populate the correct names for you.\nBut usually we want to make the columns names clean and easy to read. We can rename the columns using snake-case naming conventions while we are making selections.\n\nenergy &lt;- energy |&gt;\n  select(\n    data_year = `Data Year`,\n    BEUDO_category = `BEUDO Category`,\n    owner = Owner,\n    year_built = `Year Built`,\n    property_type = `Primary Property Type - Self Selected`,\n    ghg_emission = `Total GHG Emissions (Metric Tons CO2e)`,\n    ghg_intensity = `Total GHG Emissions Intensity (kgCO2e/ft2)`,\n    longitude = Longitude,\n    latitude = Latitude\n  ) \n\nClick the energy variable in your Environment panel to browse this smaller dataset."
  },
  {
    "objectID": "labs/lab1.html#filter-select-a-subset-of-rows",
    "href": "labs/lab1.html#filter-select-a-subset-of-rows",
    "title": "Get started with ",
    "section": "filter: Select a subset of rows",
    "text": "filter: Select a subset of rows\nNow let’s create a new dataset that only contains energy use records from MIT buildings.\ndataset |&gt; filter(&lt;condition&gt;)\nTake a look at how we achieve this using the following code:\n{r}\nenergy |&gt; \n  filter(owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\")\nViewing the result, you’ll notice that some entries are missing records for total GHG emissions, which appear as NA under the “ghg_emission” and “ghg_intensity” column. If we want to simplify the dataset by keeping only the rows with valid GHG emission records, we can apply that as a filter condition too.\nProceed to insert a new code chunk in your document like the one below. Now we are filtering MIT buildings that have emission data, and we are assigning the result to a new variable “mit_energy”.\n\nmit_energy &lt;- energy |&gt; \n  filter(owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\") |&gt; \n  filter(!is.na(ghg_emission))\n\nis.na() is a function commonly used to check whether each value in a column is missing (NA). The ! is a logical negation operator, so !is.na() checks for values that are not missing. It returns TRUE for non-missing values and FALSE for missing values."
  },
  {
    "objectID": "labs/lab1.html#summarise-create-a-summary-of-your-data",
    "href": "labs/lab1.html#summarise-create-a-summary-of-your-data",
    "title": "Get started with ",
    "section": "Summarise: Create a summary of your data",
    "text": "Summarise: Create a summary of your data\nGo ahead and run the following code and observe the result:\n\nmit_energy |&gt; \n  summarise(avg_emission = mean(ghg_emission))\n\n# A tibble: 1 × 1\n  avg_emission\n         &lt;dbl&gt;\n1        1510.\n\n\nIt calculates the average of the column “ghg_emission” of the entire dataset, and names the result “avg_emission”. The result says, of all MIT buildings, through all years, the average annual GHG emission is ~1510 MTCO2e.\nsummarise calculates summary statistics, like a total, mean, or count, across all values in the dataset. However, when used with group_by(), it calculates each group separately, collapsing each group into its own summary row.\nFor instance, below we calculate the average GHG emissions by data_year, which is the year when the energy record was taken.\n\nmit_energy |&gt; \n  group_by(year = data_year) |&gt; \n  summarise(avg_emission = mean(ghg_emission))\n\n# A tibble: 9 × 2\n   year avg_emission\n  &lt;dbl&gt;        &lt;dbl&gt;\n1  2015        1625.\n2  2016        1480.\n3  2017        1572.\n4  2018        1493.\n5  2019        1541.\n6  2020        1448.\n7  2021        1483.\n8  2022        1484.\n9  2023        1462.\n\n\nThis says, in 2015, the average annual GHG emission was ~1625 MTCO2e., and in 2016, it was ~1480 MTCO2e., so on and so forth."
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Exploratory Data Analysis with ",
    "section": "",
    "text": "This week’s Lab Exercise focuses on the dplyr package and the ggplot2 package. It also engages with data visualization best practices by demonstrating how to create and interpret a variety of graphics.\nExploratory data analysis (EDA) is a phase of a data science workflow that emphasizes getting to know the data before rushing to analyze it. EDA typically involves the creation and interpretation of summaries and graphics in order to gain insights that can inform more sophisticated analyses later on."
  },
  {
    "objectID": "labs/lab2.html#download-data-and-load-packages",
    "href": "labs/lab2.html#download-data-and-load-packages",
    "title": "Exploratory Data Analysis with ",
    "section": "Download data and load packages",
    "text": "Download data and load packages\nCreate a folder, for example, named “Lab 2”. Open RStudio and navigate to File &gt; New Project… When the dialog box will appear, choose Existing Directory. Proceed to create a new R project within your “Lab 2” folder.\nYou can create a new R file to run the code from the tutorial; you will be asked to start a Quarto Document when you begin your exercises at the end of this tutorial.\nNow navigate to Urban Institute’s website about Opportunity Zones, find the link “Download tract-level data on all Opportunity Zones”, and download this dataset to your Lab 2 project folder. Rename the file if you need.\nTo stay organized, we should load packages at the beginning of our markdown document. These are the three packages we are going to use today. You may want to run install.packages() on readxl and DataExplorer if it’s the first time you use them.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(DataExplorer)"
  },
  {
    "objectID": "labs/lab2.html#data-cleaning",
    "href": "labs/lab2.html#data-cleaning",
    "title": "Exploratory Data Analysis with ",
    "section": "Data cleaning",
    "text": "Data cleaning\nThe mutate function in dplyr allows you to modify your dataset by either adding new columns, or updating values in existing columns. It’s a very flexible function because you can transform existing variables using a wide range of operations, such as arithmetic calculations, conditional expressions, or functions.\nFor example, the Urban Institute has coded the designated variable as either taking a value of 1 when designated, or NA when not. Since the NA and 1 here have no mathematical meaning, it would be easier to read if the column simply showed text like “Designated” or “Not Designated.” In the following code, we are updating the column DesignatedOZ.\n\nozs &lt;- data |&gt;\n  mutate(DesignatedOZ =\n           ifelse(is.na(DesignatedOZ), \n                  \"not_designated\", \"designated\"))\n\nThe ifelse(condition, \"not_designated\", \"designated\") is used to set the value of DesignatedOZ based on the condition: If DesignatedOZ is NA, it assigns the text “not_designated”. Otherwise, it assigns “designated”. After the modification, we can make a quick count of both types of tracts.\n\nozs |&gt; \n  count(DesignatedOZ) \n\n# A tibble: 2 × 2\n  DesignatedOZ       n\n  &lt;chr&gt;          &lt;int&gt;\n1 designated      8764\n2 not_designated 33414\n\n\nNote: A common point of confusion is the similarity between &lt;- (assign to) and |&gt;(pipe) and when to use them. To put it briefly:\n\nWhen to Use &lt;-: Use &lt;- to save our object. In the example above, we are keeping the original data intact, but created a new object called ozs.\nWhen Not to Use &lt;-: If you only want to view results without modifying the object.\n\nThere are a few columns (such as SE_Flag) that wont’ be very helpful for this analysis. We can select a subset of columns to work on. If there is a minus sign in front of the column names, that means to drop these specified columns.\n\nozs &lt;- \n  ozs |&gt; \n  select(-c(dec_score, SE_Flag, pctown, Metro, Micro, NoCBSAType))\n\nOne of the characteristics tracked in the Urban Institute data is the median household income for each tract (medhhincome). We might want to question whether there’s a difference in the median household income for designated and not-designated census tracts.\nHowever, if you scroll down to the bottom of the dataset in the data viewer, you will notice there are quite a few of NAs in the Census demographic columns.\nHow many missing values are there, and how many would be a hurdle for my analysis? It will be great to have a sense of completeness in terms of what proportion of a field actually holds data. Below we use is.na to check if each element in ozs is NA, and use colSums to sum up all TRUE values by column.\n\ncolSums(is.na(ozs))\n\n           geoid            state     DesignatedOZ           county \n               0               23                0               98 \n            Type       Population      medhhincome      PovertyRate \n               0              112              249              141 \n       unemprate         medvalue          medrent severerentburden \n             141             1106              395              189 \n     vacancyrate         pctwhite         pctBlack      pctHispanic \n             167              131              131              131 \n    pctAAPIalone       pctunder18        pctover64        HSorlower \n             131              131              131              132 \n      BAorhigher \n             132 \n\n\nAnother way to observe missing values in each column is to use plot_missing in the DataExplorer package.\n\nDataExplorer::plot_missing(ozs)\n\n\n\n\n\n\n\n\nplot_missing calculates the proportion of missing values in a given variable, and makes some judgemental calls of whether the missing is significant, indicated by “Good”, “OK”, and “Bad”. (Feel feel to check out ?plot_missing in your console. What are the default ranges for these three categories?) Overall, most of our columns have a very small portion of missing values (less than 1%) and would not create significant representative issues. However, when performing calculations, we need to include the na.rm = TRUE argument, indicating that we are calculating based on the available 97-99%."
  },
  {
    "objectID": "labs/lab2.html#create-summary-tables",
    "href": "labs/lab2.html#create-summary-tables",
    "title": "Exploratory Data Analysis with ",
    "section": "Create Summary tables",
    "text": "Create Summary tables\nWe can calculate the average median household income for designated and not-designated census tracts. That is to collapse the stat summary of median household income summarise(mean(medhhincome)) into two groups group_by(DesignatedOZ) .\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt; \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 3\n  DesignatedOZ   Tracts Income\n  &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;\n1 designated       8764 33346.\n2 not_designated  33414 44446.\n\n\nWe can also put two columns in the group_by function, for instance, grouping first by state and then by eligibility, allowing for comparisons within each state.\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 108 × 3\n# Groups:   state [57]\n   state          DesignatedOZ   Income\n   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;\n 1 Alabama        designated     30044.\n 2 Alabama        not_designated 36542.\n 3 Alaska         designated     49840.\n 4 Alaska         not_designated 54784.\n 5 American Samoa designated       NaN \n 6 Arizona        designated     34373.\n 7 Arizona        not_designated 40961.\n 8 Arkansas       designated     31254.\n 9 Arkansas       not_designated 37814.\n10 California     designated     36134.\n# ℹ 98 more rows\n\n\n“American Samoa” might have caught our attention at this step, because we’ve got NaN (not a number), indicating that all values in this region are NA. This prompts us to return to the dataset and further clean our data.\nAre there any other states where all economic variable values are NA, possibly meaning that we have no records for tracts in those areas?\n\nozs |&gt; \n  group_by(state) |&gt; \n  summarize(all_na = all(is.na(Population))) |&gt; \n  filter(all_na == TRUE)\n\n# A tibble: 5 × 2\n  state                    all_na\n  &lt;chr&gt;                    &lt;lgl&gt; \n1 American Samoa           TRUE  \n2 Guam                     TRUE  \n3 Northern Mariana Islands TRUE  \n4 Virgin Islands           TRUE  \n5 &lt;NA&gt;                     TRUE  \n\n\nThe all(is.na(Population)) function checks if all values in the Population column for that state are NA. If they are, all_na will be TRUE. If we aim to produce economic stats, and find these states uninformative, we can choose to remove them from our dataset:\n\nozs &lt;- \n  ozs |&gt; \n  filter(!state %in% c(\"American Samoa\", \"Guam\", \"Northern Mariana Islands\",\n                       \"Virgin Islands\") & !is.na(state))\n\nThen perform the summary again:\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 103 × 3\n# Groups:   state [52]\n   state      DesignatedOZ   Income\n   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n 1 Alabama    designated     30044.\n 2 Alabama    not_designated 36542.\n 3 Alaska     designated     49840.\n 4 Alaska     not_designated 54784.\n 5 Arizona    designated     34373.\n 6 Arizona    not_designated 40961.\n 7 Arkansas   designated     31254.\n 8 Arkansas   not_designated 37814.\n 9 California designated     36134.\n10 California not_designated 50858.\n# ℹ 93 more rows\n\n\nIt might be useful for us to reshape our summary table, arranging it in a way that each state has a single row with separate columns for designated and not-designated income value.\nFunctions pivot_wider() and pivot_longer() are useful for reshaping data. pivot_wider() adds columns to a dataset by transitioning content from rows to columns. pivot_longer() does the opposite - it makes a dataset longer by transitioning columns to rows.\nIn our case, let’s use pivot_wider() to transition our Designated and Not Designated rows into columns.\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  pivot_wider(names_from = DesignatedOZ, values_from = Income)\n\n# A tibble: 52 × 3\n# Groups:   state [52]\n   state                designated not_designated\n   &lt;chr&gt;                     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Alabama                  30044.         36542.\n 2 Alaska                   49840.         54784.\n 3 Arizona                  34373.         40961.\n 4 Arkansas                 31254.         37814.\n 5 California               36134.         50858.\n 6 Colorado                 41138.         49601.\n 7 Connecticut              36760.         51389.\n 8 Delaware                 40971.         50143.\n 9 District of Columbia     38291.         62840.\n10 Florida                  31015.         40931.\n# ℹ 42 more rows\n\n\nAdd one more step, we can create a new column, to calculate and show the difference in income between designated and not designated tracts:\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  pivot_wider(names_from = DesignatedOZ, values_from = Income) |&gt; \n  mutate(Difference = designated - not_designated)\n\n# A tibble: 52 × 4\n# Groups:   state [52]\n   state                designated not_designated Difference\n   &lt;chr&gt;                     &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alabama                  30044.         36542.     -6498.\n 2 Alaska                   49840.         54784.     -4944.\n 3 Arizona                  34373.         40961.     -6588.\n 4 Arkansas                 31254.         37814.     -6560.\n 5 California               36134.         50858.    -14724.\n 6 Colorado                 41138.         49601.     -8463.\n 7 Connecticut              36760.         51389.    -14628.\n 8 Delaware                 40971.         50143.     -9172.\n 9 District of Columbia     38291.         62840.    -24548.\n10 Florida                  31015.         40931.     -9916.\n# ℹ 42 more rows"
  },
  {
    "objectID": "labs/lab2.html#distribution-of-one-variable",
    "href": "labs/lab2.html#distribution-of-one-variable",
    "title": "Exploratory Data Analysis with ",
    "section": "Distribution of one variable",
    "text": "Distribution of one variable\n\nBoxplot\nThe code below creates a boxplot to contrast the distribution of poverty rates between designated opportunity zones and undesignated zones. We are using grammars of the ggplot function introduced in class, then adding more features with the + operator and other functions listed in the package reference.\n\nozs |&gt; ggplot(): This is the main plotting function. ozs is your dataset we use.\ngeom_boxplot(): Recall that geometric layers are called geoms_*. It tells R what kind of geometry you want to use visualize the data.\naes(x = DesignatedOZ, y = PovertyRate): The aes() function is where you tell ggplot which variable goes on the x axis followed by which variable goes on the y axis.\nThe third aesthetic element is fill, which indicates the filled color of the boxplot. Accordingly, we use the fill argument in the labs function to set the name of the legend.\nWe used a new function scale_y_continuous to specify y axis properties. Here we are making sure the poverty rate are labeled as percentages. If you remove this line, they will by default show as decimal numbers.\n\n\nozs |&gt; \n  ggplot(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ)) +\n  geom_boxplot() + \n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Opportunity Zone Eligible Tracts\", y = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\n\n\n\n\nBy comparing the 50th percentile (the horizontal line inside each box) we can see that tracts designated as Opportunity Zones have a higher poverty rate compared with those not designated. The heights of the boxes themselves give us an indication of how closely around the median all values in the dataset are concentrated—the degree of dispersion or spread. The vertical lines are called whiskers and extend upward and downward to the lowest values that are not candidates for outlier status. An outlier is an unusual value that could potentially influence the results of an analysis. These are indicated with dots in the boxplot.\n\n\nDensity plot\nBy modifying the last code chunk, we can make a density plot to describe the distribution of poverty rate. A density plot can be understood as a smoothed version of the histogram, and provides a more direct view of of the shape and peaks in the data. The x-axis typically represents the range of values for the variable of interest, while the y-axis represents the probability density (how likely it is for the variable to take on a particular value within that range).\n\nozs |&gt; \n  ggplot(aes(x = PovertyRate, fill = DesignatedOZ)) +\n  geom_density() + \n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\n\n\n\n\nIf you have noticed - in the code above, we didn’t provide a variable that what goes to the y-axis. Where does the value “density” come from?\nMany graphs, like boxplot, plot the raw values of your dataset. But other graphs, like histograms and density plots, calculate new values to plot. Here a density takes the count of data points at discrete poverty rate levels and smooths it out into a continuous curve. Then calculated values (probability density) go to the y-axis.\n\n\nCombinations of basic graphs to create composite views\nOne of the coolest thing about ggplot is that we can plot multiple geom_ on top of each other. For instance, we can combine the two plots above, to show both visually appealing curves and essential statistics (medians, quartiles, outliers, etc.) The following code uses two geom_(Check out geom_violin for more!), and introduces several new arguments for fine-tuning the cosmetics.\n\ntrim = FALSE: If TRUE (default), trim the tails of the violins to the range of the data. If FALSE, don’t trim the tails and show the complete distribution.\nalpha = 0.5: the transparency of the plotting area.\ncoord_flip(): whether the y axis is displayed horizonally or vertically.\nlegend.position = \"none\": the position of legend (“left”, “right”, “bottom”, “top”, or two-element numeric vector), or not showing the legend (“none”).\n\n\nozs |&gt; ggplot() +\n  geom_violin(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ), trim = FALSE, alpha = 0.5) +\n  geom_boxplot(aes(x = DesignatedOZ, y = PovertyRate), color = \"black\", width = .15, alpha = 0.8) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Opportunity Zone Eligible Tracts\",\n    y = \"Poverty Rate\",\n    title = \"Distribution of Poverty Rate\"\n  ) +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nA useful way to learn new arguments in ggplot is to take some of them out and see how it changes the plot."
  },
  {
    "objectID": "labs/lab2.html#relationship-between-two-variables",
    "href": "labs/lab2.html#relationship-between-two-variables",
    "title": "Exploratory Data Analysis with ",
    "section": "Relationship between two variables",
    "text": "Relationship between two variables\n\nScatter Plot\nWe are often interested in bivariate relationships or how two variables relate to one another. Scatterplots are often used to visualize the association between two continuous variables. They can reveal much about the nature of the relationship between two variables.\nLet’s use your subset of Massachusetts data to perform this part of analysis. (We could use the nationwide dataset, but there will be over 40,000 points showing on the graph, which will not be pleasing to the eye).\n\nozs_ma &lt;- ozs |&gt; filter(state == \"Massachusetts\") \n\nWe begin by creating a scatterplot of poverty rate and racial distribution. Note that we used theme_bw, which is a theme template for a cleaner look.\n\nozs_ma |&gt; \n  ggplot(aes(x = pctBlack, y = PovertyRate)) +\n  geom_point() +\n  labs(x = \"Proportion of Black Population\",\n       y = \"Poverty rate\",\n       title = \"Poverty rate vs. proportion of black population in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nThere is a slight increase of slope as we move from left to right along the x-axis. However, there are all the points shown here. How can we distinguish between the two groups - Add a third “aesthetic element”, which is DesignatedOZ, and include the linear regression lines using geom_smooth.\n\nozs_ma |&gt; \n  ggplot(aes(x = pctBlack, y = PovertyRate, color = DesignatedOZ)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Proportion of Black Population\",\n       y = \"Poverty rate\",\n       title = \"Poverty rate vs. proportion of black population in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()"
  },
  {
    "objectID": "labs/lab2.html#exercise-1",
    "href": "labs/lab2.html#exercise-1",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet’s first create some summary tables to analyze opportunity zones in Massachusetts. Please include the code you used, the tables you produced, and any explanatory text that you think would help clarify your results:\n\nSuppose you are interested in poverty rates. In Massachusetts, what are the average poverty rates for Opportunity Zones and non-Opportunity Zones?\nWhen you have the result of Question 1 (the summary for Massachusetts), what are the corresponding situations by county in Massachusetts?\nReorganize your previous table, which county has the greatest disparity in poverty rate between designated and non-designated tracts?"
  },
  {
    "objectID": "labs/lab2.html#exercise-2",
    "href": "labs/lab2.html#exercise-2",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 2",
    "text": "Exercise 2\nFocus on your data in Massachusetts, now choose from the following variables: medhhincome, vacancyrate, unemprate, pctwhite, pctblack, pctHispanic, pctover64, HSorlower to help answer the following questions.\n\nSelect one of the variables, create a graphical representation that contrasts its distribution in designated tracts and in undesignated tracts in Massachusetts.\nSelect two variables, create a graphical representation that describes how they relate (or don’t relate) to each other, including the direction of this relationship.\nWhat can we say about the difference in demographic/economic conditions reflected by these graphs between designated and not designated tracts? Include in your document a few sentences of write-up. You can connect your findings with your summary tables above, and with some broader discussions about Opportunities Zones found here."
  },
  {
    "objectID": "labs/lab2.html#exercise-3",
    "href": "labs/lab2.html#exercise-3",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 3",
    "text": "Exercise 3\nIn this part, we will make a Bar Chart. First, let’s use our familiar group_by + summarise process to calculate the average median house income by county in Massachusetts.\n\nozs_ma |&gt; \n  group_by(county, DesignatedOZ) |&gt;  \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) \n\n# A tibble: 25 × 3\n# Groups:   county [13]\n   county            DesignatedOZ   Income\n   &lt;chr&gt;             &lt;chr&gt;           &lt;dbl&gt;\n 1 Barnstable County designated     46717.\n 2 Barnstable County not_designated 61663.\n 3 Berkshire County  designated     35199 \n 4 Berkshire County  not_designated 51122.\n 5 Bristol County    designated     34573.\n 6 Bristol County    not_designated 42035.\n 7 Dukes County      not_designated 46816 \n 8 Essex County      designated     41358.\n 9 Essex County      not_designated 49966.\n10 Franklin County   designated     41711.\n# ℹ 15 more rows\n\n\nPlease pipe your summarized table to ggplot() for visualization. The geom function you should use here is geom_col.\n\nTake a few minutes to compare the bar chart you created and the one below:\n\n\nThere should be a few differences, which have enhanced the overall quality. How can you modify your code to replicate the bar chart in this image? In a new code chunk, please copy and paste your last bar chart code, and try your best to address the following questions.\n\nThe bars are put side-by-side instead of stacking on top of one another. If you don’t want a stacked bar chart, you can use the position argument in geom_col. There will be three options: “identity”, “dodge”, or “fill”.\nThe x-axis labels are titled to 45 degrees. How can I achieve this? Hint.\nThe labels on the y-axis are formatted in thousands with commas. This can be achieved by modifying the function scale_y_continuous(labels = scales::percent) we have seen above. Hint.\nLastly, the counties are not arranged alphabetically, but rather by the income values mapped to the y-axis, starting from large to small. How can I achieve this? Hint.\nPlease add the title, subtitle, x- and y-axis labels, and the data source annotation to your bar chart.\nPlease choose a theme template for your bar chart.\n\nFeel free to consult the R Graph Gallery and Aesthetic specifications for additional resources."
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Describe neighborhood dynamics with ",
    "section": "",
    "text": "In this lab, we will develop a place profile to quantify the physical and environmental characteristics of a given area. Planners are usually interested in understanding and enhance improving urban environments. Our approach will involve utilizing spatial data and conducting spatial analysis.\n\nTo put things into context, we will describe pedestrian-friendly built environment in Boston neighborhoods. Urban factors such as the presence of walking facilities, the density and variety of businesses, the number of residents and jobs, and streetscape elements like benches, storefronts, and shade all influence the extent and nature of walkable areas (Lai & Kontokosta, 2018; Frank et al., 2006; Owen et al., 2007; Ewing & Cervero, 2010).\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(osmdata)\nlibrary(DT)\nlibrary(scales)\n\nFor the purpose of demonstrating how to gather, combine and analyze spatial data from different sources, we are going to develop three relatively simple indicators to measure the pedestrian environment in a Boston neighborhoods: sidewalk density, number of restaurants measured by points of interest (POI), and tree canopy coverage."
  },
  {
    "objectID": "labs/lab4.html#calculate-feature-length",
    "href": "labs/lab4.html#calculate-feature-length",
    "title": "Describe neighborhood dynamics with ",
    "section": "Calculate feature length",
    "text": "Calculate feature length\nFrom the front page of Analyze Boston, search for “sidewalk”, you will find the Sidewalk Centerline data. Download the Shapefile to your data folder, unzip it, and then use st_read() to read it into R and assign it to a variable (named e.g. “sidewalk”).\nCheck the results by clicking sidewalk in the Environment panel, or equivalently, using View(sidewalk). Details of each line are stored in the geometry attribute. While these details aren’t meant for humans to read directly, machines can interpret it and perform operations, such as calculating areas for polygons and length for lines:\n\nsidewalk |&gt; mutate(length = st_length(geometry))\n\nSimple feature collection with 110031 features and 4 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 742172.1 ymin: 2909272 xmax: 811805.7 ymax: 2969957\nProjected CRS: NAD83 / Massachusetts Mainland (ftUS)\nFirst 10 features:\n   OBJECTID     TYPE ShapeSTLen                       geometry\n1         1 CWALK-CL  64.029105 LINESTRING (755403.2 295473...\n2         2 CWALK-CL  48.941826 LINESTRING (757458.1 295887...\n3         3 CWALK-CL  27.003284 LINESTRING (754873.1 295631...\n4         4 CWALK-CL  50.113763 LINESTRING (756019.7 295713...\n5         5 CWALK-CL  33.144903 LINESTRING (755399.5 295530...\n6         6 CWALK-CL  51.157055 LINESTRING (756373.4 295663...\n7         7 CWALK-CL  59.903693 LINESTRING (756988.7 295606...\n8         8 CWALK-CL  77.648147 LINESTRING (756633.2 295547...\n9         9 CWALK-CL   5.862916 LINESTRING (758622.5 295417...\n10       10 CWALK-CL  44.308273 LINESTRING (760087.9 295349...\n                       length\n1  64.029105 [US_survey_foot]\n2  48.941826 [US_survey_foot]\n3  27.003284 [US_survey_foot]\n4  50.113763 [US_survey_foot]\n5  33.144903 [US_survey_foot]\n6  51.157055 [US_survey_foot]\n7  59.903693 [US_survey_foot]\n8  77.648147 [US_survey_foot]\n9   5.862916 [US_survey_foot]\n10 44.308273 [US_survey_foot]\n\n\nWe have an additional column, length , that stores the length of each segment. These lengths are calculated in feet because the neighborhood shapefile uses EPSG code 2249, which represents the “State Plane coordinate system Massachusetts Mainland (ftUS)” projection. State Plane projections are “projected”, meaning that x and y coordinates are measured in linear units (feet, in our case).\n\nst_crs(sidewalk)$epsg  \n\n[1] 2249\n\nst_crs(sidewalk)$input\n\n[1] \"NAD83 / Massachusetts Mainland (ftUS)\"\n\n\nSecond, the values are “unit” objects. Although we can do basic math operations on unit objects, we typically convert them to simple numeric values using as.numeric():\n\nsidewalk |&gt; mutate(length = as.numeric(st_length(geometry)))\n\nSimple feature collection with 110031 features and 4 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 742172.1 ymin: 2909272 xmax: 811805.7 ymax: 2969957\nProjected CRS: NAD83 / Massachusetts Mainland (ftUS)\nFirst 10 features:\n   OBJECTID     TYPE ShapeSTLen                       geometry    length\n1         1 CWALK-CL  64.029105 LINESTRING (755403.2 295473... 64.029105\n2         2 CWALK-CL  48.941826 LINESTRING (757458.1 295887... 48.941826\n3         3 CWALK-CL  27.003284 LINESTRING (754873.1 295631... 27.003284\n4         4 CWALK-CL  50.113763 LINESTRING (756019.7 295713... 50.113763\n5         5 CWALK-CL  33.144903 LINESTRING (755399.5 295530... 33.144903\n6         6 CWALK-CL  51.157055 LINESTRING (756373.4 295663... 51.157055\n7         7 CWALK-CL  59.903693 LINESTRING (756988.7 295606... 59.903693\n8         8 CWALK-CL  77.648147 LINESTRING (756633.2 295547... 77.648147\n9         9 CWALK-CL   5.862916 LINESTRING (758622.5 295417...  5.862916\n10       10 CWALK-CL  44.308273 LINESTRING (760087.9 295349... 44.308273"
  },
  {
    "objectID": "labs/lab4.html#spatial-intersection",
    "href": "labs/lab4.html#spatial-intersection",
    "title": "Describe neighborhood dynamics with ",
    "section": "Spatial intersection",
    "text": "Spatial intersection\nNow we are going to perform intersection. st_intersection() finds the shared part of two spatial objects. If we overlay sidewalks with the entire neighborhoods, it splits sidewalks at neighborhood boundaries. Each split segment is linked to the attributes of the neighborhood it falls within.\nBut the two spatial files need to have the same projection (i.e. they need to spatially line up) to perform the intersection. Check the CRS of neighborhood, it’s EPSG: 4326, which is an unprojected system where locations are stored in longitude and latitude. Will first use st_transform to convert it to the same projection as the sidewalks, EPSG: 2249.\n\nneighborhood &lt;- st_transform(neighborhood, crs = 2249)\n\nThen perform the intersection.\n\nsidewalk_data &lt;- \n  st_intersection(sidewalk, neighborhood)\n\nIt takes a few seconds to run, but when it’s finished, you will see that neighborhood names are attached in sidewalk_data! Then we can calculate the length of each split sidewalk segment.\n\nsidewalk_data &lt;- sidewalk_data |&gt; \n  mutate(length = as.numeric(st_length(geometry))) \n\nWith neighborhood names here in the table, we will group_by() neighborhood and summarise() the total sidewalk length.\n\nsidewalk_data &lt;- sidewalk_data |&gt; \n  group_by(nbh_name) |&gt; \n  summarise(sidewalk_length = sum(length)) |&gt; \n  st_drop_geometry()\n\nI’ve dropped the geometry so we have a plain table. When it’s finished, you can View(sidewalk_data) . It has 24 rows, showing the total sidewalk length (in feet) in each neighborhood."
  },
  {
    "objectID": "labs/lab4.html#calculate-the-number-of-pois-by-neighborhood",
    "href": "labs/lab4.html#calculate-the-number-of-pois-by-neighborhood",
    "title": "Describe neighborhood dynamics with ",
    "section": "Calculate the number of POIs by neighborhood",
    "text": "Calculate the number of POIs by neighborhood\nWe now have a point shapefile (restaurants) and a polygon shapefile (neighborhood), and we want to count the number of points in each neighborhood. They are points, so we don’t need to look for overlapping/intersecting parts, st_join() will be sufficient.\n\nrestaurant_data &lt;- \n  st_join(restaurants, neighborhood) \n\nTake a look at what we’ve got. It’s still the same set of restaurants but each one has neighborhood information joined to it. Remove those fall outside of our neighborhood boundary, then count by neighborhood:\n\nrestaurant_data &lt;- \n  restaurant_data |&gt; \n  filter(!is.na(nbh_name)) |&gt; \n  count(nbh_name, name = \"restaurant\") |&gt; \n  st_drop_geometry()"
  },
  {
    "objectID": "labs/lab6.html",
    "href": "labs/lab6.html",
    "title": "Creating Interactive Maps and Dashboard with ",
    "section": "",
    "text": "Today we’re going to explore some online maps and dashboard. We will continue to use an example of analyzing Airbnb data in Chicago to illustrate how to use these tools effectively.\n\nInteractive maps with leaflet\nIntroduce dashboards with flexdashboard\n\nCreate a project folder to hold your files. Once set up, I recommend starting your code in a simple R file to follow along with the tutorial. Later, you’ll be asked to identify which parts of the code are for the map and which are for the graph for submissions.\n\n# You may need to install plotly, leaflet, flexdashboard\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(plotly)\nlibrary(leaflet)\nlibrary(flexdashboard)"
  },
  {
    "objectID": "labs/lab6.html#work-process",
    "href": "labs/lab6.html#work-process",
    "title": "Creating Interactive Maps and Dashboard with ",
    "section": "Work Process",
    "text": "Work Process\n\nBasemap\nFirst, we initiate leaflet and add a basemap. With addTiles() you will add the default base map, and with setView() you will be able to set a center point and a zoom level. Run the following code to set our view to Chicago.\nleaflet() |&gt;\n  addTiles() |&gt;\n  setView(lng = -87.636483, lat = 41.862984, zoom = 10) \nBy default, addTiles() generates a basemap using OpenStreetMap tiles. They’re suitable, but the options of which basemap to use are extensive. Check out a lengthy list here. You can use addProviderTiles() instead to pick from among a set of third-party options. I’m partial to the washed simplicity of the “CartoDB.Positron” tiles, so I’ll use those in the maps that follow.\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,zoom = 10) \n\n\n\n\n\n\n\nCircle markers\nThen we can add all Airbnb listings as “circle markers” on the map. Circle markers offer more styling options than simple points, allowing you to define attributes such as: circle size (radius), whether to have storkes (stroke=TRUE) or fill (fill=TRUE), with what fill color (fillColor) and transparency (fillOpacity), etc.,\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt; \n  addCircleMarkers(data = airbnb,\n                   fill = TRUE,\n                   fillOpacity = 0.5,\n                   stroke = FALSE,\n                   radius = 1) \n\n\n\n\n\n\n\nColor\n\nFunction-Based Color Mapping\nRather than showing all listings in one color, we might want to differentiate listings by, for example, room type.\nIn ggplot2, when you specify color = &lt;variable name&gt;, ggplot automatically map colors based on your variable. However, Leaflet doesn’t know how to handle colors unless you tell it. You will need to create a color palette (pal) using color mapping functions to explicitly define how values in your variable correspond to colors.\n\n\nDefine pal using color mapping functions:\nIn the following line of code, we are defining our color palette pal using a color mapping function colorFactor(). This builds a relationship between the categorical values in the airbnb$room_type variable and a color ramp (“RdYlGn” – red, yellow, green).\npal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\nWe used colorFactor() because variables in room_type are categorical. For numeric data, we will instead use colorBin() , which divides the range into “equal intervals”; and colorQuantile() , which creates quantiles for varying values.\n\n\nApplying pal to Map Elements\nOnce defined, you can use pal in multiple places on your map to consistently apply the color mapping. In the following code, the palette is passed to two things:\n1) fillColor = ~pal(room_type) - so that each room type will be filled with its corresponding color. The ~ is telling the system to treat room_type as a column of the airbnb dataset rather than a separate object.\n2) addLegend(pal = pal) so the that colors in the legend will show up accordingly.\n\npal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt;\n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1) |&gt;\n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  )\n\n\n\n\n\n\n\n\nPolygons\nShapefiles can be added to leaflet through addPolygons() . For now, let’s download the Chicago boundary shapefile here. Once it’s downloaded, read it into R using appropriate sf functions.\nIn the last lab, we have been transforming all shapes to 2249, because we were performing spatial calculations, and need to use a local CRS like 2249 for Massachusetts to minimize distortion. For web mapping, however, EPSG:4326 (latitude/longitude) is always used to ensure global compatibility, as it’s the standard for GPS-based systems.\n\nchi_bnd &lt;- \n  st_read(\"../data/Chicago_City_Limits.shp\") |&gt; # change this to your path\n  st_transform(4326)\n\nNow we will add Chicago boundary to our map, while also adjusting the boundary color, width, and setting the fill color to transparent.\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt;\n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1) |&gt;\n  addPolygons(data = chi_bnd,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1) |&gt;\n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  )\n\n\n\n\n\n\n\nPopup labels\nPop-ups show information when clicking a map item. In our example, each circle marker represents one Airbnb listing. We can request the map to show an attribute of this listing (such as name) when clicking it.\n\naddCircleMarkers(data = airbnb,\n                 fillOpacity = 0.5,\n                 stroke = FALSE,\n                 radius = 1,\n                 popup = ~name) \n\nOr, displays more than one attributes, such as name, host_name and price:\n\naddCircleMarkers(data = airbnb,\n                 fillOpacity = 0.5,\n                 stroke = FALSE,\n                 radius = 1,\n                 popup = ~paste(name, host_name, price)) \n\nSame as before, we use ~ to indicate these are data attributes rather than individual objects.\nTry adding the popup argument to your addCircleMarkers() function. You will see that information are displayed in one continuous row, which is not easy to read. But we can fix this by defining a label format outside of the plotting process. We can even control the appearance of pop-up information using line breaks &lt;br&gt; or making text bold. For example:\n\npopup_format &lt;-\n  paste0(\"&lt;b&gt;Name:&lt;/b&gt;\", airbnb$name, \"&lt;br&gt;\",\n         \"&lt;b&gt;Host Name: &lt;/b&gt;\", airbnb$host_name,\"&lt;br&gt;\",\n         \"&lt;b&gt;Price: &lt;/b&gt;\", \"$\", airbnb$price, \"&lt;br&gt;\"\n  )\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt;\n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1,\n                   popup = popup_format) |&gt;\n  addPolygons(data = chi_bnd,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1) |&gt;\n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  )\n\n\n\n\n\nWhat we have created so far already resembles the map on Inside Airbnb’s visualization. For the last step, we will add a layer control to turn on and off layer groups as we wish!\n\n\nLayer Control\nIn the following code, we are assigning each item we added into a group. Then these group names are passed to the addLayersControl() for users to toggle on and off.\nAdd these layer control lines to your code - be careful to match all parentheses correctly.\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron,\n                   group = \"CartoDB Positron\") |&gt;\n  addProviderTiles(\"Esri.WorldImagery\", \n                   group = \"ESRI World Imagery\") |&gt;\n  addProviderTiles(providers$CartoDB.DarkMatter, \n                   group = \"CartoDB Dark\") |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt;\n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1,\n                   popup = popup_format, \n                   group = \"Airbnb Listings\") |&gt;\n  addPolygons(data = chi_bnd,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1, \n              group = \"Chicago Boundary\") |&gt;\n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\") |&gt; \n  addLayersControl(\n    baseGroups = c(\"CartoDB Positron\", \n                   \"ESRI World Imagery\",\n                   \"CartoDB Dark\"),\n    overlayGroups = c(\"Airbnb Listings\", \"Chicago Boundary\")\n  )\n\n\n\n\n\nNow, you should have a complete code for the Leaflet map. Make sure it runs properly and produces the map you want. You can then save it as a file, such as leaflet.R."
  },
  {
    "objectID": "labs/lab6.html#room-type",
    "href": "labs/lab6.html#room-type",
    "title": "Creating Interactive Maps and Dashboard with ",
    "section": "Room Type",
    "text": "Room Type\nThis graph displays the median price by room type. It starts with a static ggplot graph, which is then passed to ggplotly for interactivity.\nNote that you can reuse the pal you have defined before, to make the room type color code consistent between the map and the graph.\n\nairbnb |&gt;\n  count(room_type) |&gt; \n  plot_ly(\n  x = ~n,\n  y = ~reorder(room_type, n),  # use reorder just in case\n  type = \"bar\",\n  orientation = \"h\",\n  marker = list(color = pal(airbnb$room_type)),\n  hoverinfo = \"x\"  # 2) Only show value when hovering\n) |&gt; \n  layout(\n    xaxis = list(title = \"listings\"),\n    yaxis = list(title = \"\", showticklabels = TRUE) \n  )"
  },
  {
    "objectID": "labs/lab6.html#proportion-of-airbnb-listings-by-type",
    "href": "labs/lab6.html#proportion-of-airbnb-listings-by-type",
    "title": "Creating Interactive Maps and Dashboard with ",
    "section": "Proportion of Airbnb Listings by Type",
    "text": "Proportion of Airbnb Listings by Type\nThere’s one type of graph where I’d go directly to plotly: pie or donut charts. ggplot2 is less flexible with circular graphs since it’s more focused on x and y axes.\nThis graph shows the count of listings for each room type. The room type goes to labels(read: groups or legend items) and the number of listings for each type goes to values.\n\nairbnb |&gt; \n  count(room_type) |&gt; \n  plot_ly() |&gt;  \n  add_pie(labels = ~room_type, \n          values = ~n,\n          hole = 0.6)\n\n\n\n\n\nFor your interest in digging deeper into plotly, check out plotly R graph library and its ggplot2 integration examples."
  },
  {
    "objectID": "labs/lab6.html#populate-a-dashboard-template",
    "href": "labs/lab6.html#populate-a-dashboard-template",
    "title": "Creating Interactive Maps and Dashboard with ",
    "section": "Populate a dashboard template",
    "text": "Populate a dashboard template\nJust below the YAML header of flexdashboard.Rmd, there is a code chuck named setup and marked include = FALSE. Here you can supply any data related to package loading and data preparation. Make sure you include here any of your scripts related to loading packages and reading data:\n\nNow you only need to identify and isolate the code that we produced today to populate the respective three chart sections.\nIn other words, you should copy the code you’ve worked through building a leaflet map, and paste them in the blank code chunk under “Chart A”.\nThen copy and paste the codes under “Median Room Price by Type” and “Proportion of Airbnb Listings by type” to the flexdashboard sections “Chart B” and “Chart C”.\nWhen you are ready, knit the document again, and a dashboard should appear!\nFor your reference, I have my flexdashboard.Rmd uploaded here."
  },
  {
    "objectID": "labs/lab8.html",
    "href": "labs/lab8.html",
    "title": "Multivariate Regression and Model Selection with ",
    "section": "",
    "text": "This week begins our journey into machine learning. Ken Steif’s book chapter walks through an example using Boston housing data, showing how to build, apply, and evaluate a predictive model for home prices. We will build our study on this example, and continue to explore more models.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(caret)"
  },
  {
    "objectID": "labs/lab8.html#get-to-know-the-variables",
    "href": "labs/lab8.html#get-to-know-the-variables",
    "title": "Multivariate Regression and Model Selection with ",
    "section": "Get to know the variables",
    "text": "Get to know the variables\nWe’ll be working with the CSV dataset containing sale prices and housing characteristics for homes sold in Boston between August 2015 and August 2016. The data used in this tutorial is a subset of a larger dataset assembled from here. You can explore the full data dictionary here.\nRead in the dataset using read_csv() and take an initial look using functions like glimpse()\n\nboston &lt;- read_csv(\"../data/boston_house_price_collapse.csv\")\n#head(boston)\n\nA few notes:\n\nThe SalePrice variable is the target we want to predict.\nOther columns describe building characteristics — such as square footage, number of floors, and number of bedrooms.\nSome variables are numeric (dbl), while others are categorical (chr or fct).\n\nQuestion: Based on your first observation, which variables do you think might be strong predictors of house price? Which ones seem less useful?"
  },
  {
    "objectID": "labs/lab8.html#eda-and-data-processing",
    "href": "labs/lab8.html#eda-and-data-processing",
    "title": "Multivariate Regression and Model Selection with ",
    "section": "EDA and data processing",
    "text": "EDA and data processing\nBefore feeding any data into a model, we need to understand and clean it. Every dataset is different, but there are some key steps you should always consider:\n\nUnderstand data structure: str(), glimpse() are useful to get to know the details of variables we are dealing with. Numeric and categorical variables require different treatment in a model later on so this is something to keep in mind. We will also see in our case, parcel_no is just an identifier and is not useful for predicting price.\nHandle missing values. is.na(), sum(is.na(...)), colSums(is.na(...)) can help identify which variables have missing values and how many. But how will deal with them (drop, fill, or flag) will depend on variable contexts and further inspection.\nLook at distributions. This is an important step before fitting a regression model because it helps guide feature selection and transformation. We will take a closer look at how these relationships show up in different types of variables:\nContinuous variables\n\n\nboston |&gt; \n  select(SalePrice, PricePerSq, LivingArea, GROSS_AREA, NUM_FLOORS, R_BDRMS) |&gt; \n  pairs()\n\n\n\n\n\n\n\n\nThe pairs() function creates a pairwise scatter plot matrix, which lets you visually examine the relationships between multiple variables at once. In the graph, we usually look for the following:\n\nTop row (SalePrice as Y-axis):These show how each predictor relates to the outcome. If you see an upward trend (a roughly linear increase with SalePrice), that predictor is likely to be a strong predictor of price.\nRelationships between predictors: Look for predictors that are highly correlated with each other (a sign of multicollinearity). For instance, LivingArea and GrossArea may appear nearly linear in their scatterplot. If you calculate the Pearson correlation, it’s about 0.94. This means they contain overlapping information.\n\nDepending on the modeling method (e.g., linear regression vs. tree-based methods), multicollinearity may or may not be a serious concern, but it’s always worth noting.\n\nPattens, outliers and clusters. These visual cues in pairwise plots help you decide whether a linear model is appropriate, and whether certain data points might influence your results. For example:\n\nThe variable PricePerSq may look odd with a bunch of y values at x=0. PricePerSq here is a derived variable, calculated as SalePrice / LivingArea. Since it’s directly related to the outcome, it should not be used as a predictor of SalePrice .\nYou’ll also notice clear outliers especially for luxury homes that have extremely large sale prices. This should match what you would see in the histogram of SalePrice. Most homes are priced under $1 million, but a small number of high-end homes stretch the distribution far to the right.\n\n\n\nCategorical variables\nYou can also try making scatterplots with categorical variables (e.g., by including them in a pairs() plot). But the results often look strange or uninformative, because points just line up vertically at two or three x-values. A better approach is to compare the average outcome across the categories. For example, you can group the data by a variable like Style and compute the mean Saleprice for each category:\n\nboston |&gt; \n  select(SalePrice, Style, OWN_OCC, R_BLDG_STY, R_ROOF_TYP) |&gt; \n  filter(SalePrice &lt;= 1000000) |&gt; \n  pivot_longer(-SalePrice, names_to = \"Variable\", values_to = \"Value\") |&gt;  \n  group_by(Variable, Value) |&gt; \n  summarise(mean_price = mean(SalePrice, na.rm = TRUE)) |&gt; \n  \n  ggplot() +\n    geom_col(aes(x = Value, y = mean_price),position = \"dodge\") +\n    facet_wrap(~ Variable, scales = \"free_x\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(y = \"Mean Sale Price\", x = \"Category\", title = \"Mean SalePrice by Categorical Variables\")\n\n\n\n\n\n\n\n\nWhen examining categorical variables, we’re not looking for a linear trend, because the categories don’t have a natural order or numeric spacing. Instead, we focus on two main things:\n\nRare or problematic categories. Just like with missing values in numeric variables, rare categories can cause problems in modeling. For example, if we look at the Style variable, we might see a category labeled “Unknown” and it only appears twice in the dataset. Such categories don’t offer enough data to reliably estimate their effect, may introduce noise, and might need to be removed or combined.\nDifferences between groups. We want to know if the categories differ meaningfully in the outcome variable. If all the categories have similar average sale prices, it means the variable doesn’t explain much of the variation in price and is likely a weak predictor. In our case, OWN_OCC seems to have little impact on sale price."
  },
  {
    "objectID": "labs/lab8.html#model-interpretation",
    "href": "labs/lab8.html#model-interpretation",
    "title": "Multivariate Regression and Model Selection with ",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nCoefficients: Effect Size\n\nThe Estimate tell us how each predictor is associated with SalePrice, assuming all other variables are held constant. The sign of the values (+ or -) tell you the direction of the relationship. The Standard Error is about how certain we are about the estimate, smaller values mean more confidence.\n\nThen we might see a few issues: GROSS_AREA is negatively associated with housing price, so is R_BDRM, which doesn’t make intuitive sense. This relates back to the multicollinearity issue we’ve spotted. When this happens, the model struggles to disentangle their individual effects.\n\nFor continuous variables, the values of Estimate represent how much SalePrice changes for a 1-unit increase in the predictors. If LivingArea has a coefficient estimate of 500, it means for each additional square foot, price increases by $500, assuming everything else stays the same.\nFor categorical variables, one category is used as the reference group, and the others are compared against it. For example, Style includes 11 types of housing style, R will pick one (like “Cape”) as the base. if the coefficient for \"Colonial\" is -190,000, it means homes with the \"Colonial\" style are predicted to cost $190,000 less than \"Cape\" style homes, on average.\n\nCoefficients: Significance\n\nThe t-statistics and its mirror image, the p-value, measures the extent to which a coefficient is “statistically significant”, in other words, whether the relationship we see between that predictor and the outcome is likely real, or could have happened just by random chance.\nThe higher that t-statistics (and the lower the p-value), the more significant the predictor. Since parsimony is a valuable model feature, it is useful to have a tool like this to guide choice of variables to include as predictors.\n\nModel performance\n\nModel performance means how well our regression model fits the data and how useful it is for making predictions. We have several key metrics to evaluate this:\nThe coefficient of determination (R²) tells us the proportion of variance in the outcome that is explained by the predictors, ranging from 0 (no explanatory power) to 1 (perfect fit).\nThe residual standard error (RSE) measures the average size of the prediction errors of the outcome. Lower values indicate a better fit.\nThe model p-value, derived from an F-test, tells us whether the model as a whole is statistically significant, i.e. whether it does better than a model with no predictors.\nWith that, we can see our linear model explains 57% of the variation in sale prices, which is a decent but not great fit. On average, data points are $396,300 away from the values on the regression line."
  },
  {
    "objectID": "labs/lab8.html#machine-learning-workflow",
    "href": "labs/lab8.html#machine-learning-workflow",
    "title": "Multivariate Regression and Model Selection with ",
    "section": "Machine Learning Workflow",
    "text": "Machine Learning Workflow\nI will go through a full modeling workflow using the caret package. caret lets you build many models (lm, glmnet, rf, svm, xgbTree, etc.) using a consistent syntax.\nStep 1: Preprocessing\n\nRemove irrelevant columns, such as IDs.\nDrop or combine categories with very few cases.\nHandle outliers, especially for skewed variables like SalePrice.\n\nOption 1: Drop extremely high values (e.g., over $5M).\nOption 2: Log-transform SalePrice, but that changes interpretation. For simplicity, we’ll just drop a few extreme outliers here.\n\n\n\nmodeldata &lt;- boston |&gt; \n  select(-c(Parcel_No, PricePerSq, LU, R_BLDG_STY,\n                                  Latitude, Longitude))  |&gt; \n  filter(Style != \"Unknown\") |&gt; \n  filter(SalePrice &lt; 5000000)\n\nStep 2: Split data into training and testing sets\nThis involves first applying createDataPartition to generate row indices for the training set, then using those row indices to isolate the two subsets.\n\n# Set a fix \"random seed\" to make the split reproducible\nset.seed(42)  \n\n# Generate row indices for the training set\ntrain_index &lt;- createDataPartition(\n  y = modeldata$SalePrice, # stratify based on the SalePrice variable\n  p = 0.7,                 # we want 70% of the data in the training set.\n  list = FALSE             # makes sure the output is a vector, not a list, so it's easier to use for indexing.\n)\n\n# Isolate the two subsets\ntrain_data &lt;- modeldata[ train_index,]\ntest_data  &lt;- modeldata[-train_index,]\n\nStep 3: Build the model\nIn the following code, you can swap out the model by simply changing the argument for method= , such as linear regression (\"lm\"), random forest (\"rf\"), elastic net (\"glmnet\"), XGBoost (\"xgbTree\"), etc.\nAnother important point in this model is that we’ve set up cross-validation using trainControl(). Cross-validation means we’re splitting the train_data into smaller parts during training to better evaluate model performance. Specifically, with 5-fold cross-validation (number = 5):\n\nEach time, one part of the data is set aside as a validation set, and the model is trained on the remaining parts.\nThen, the model is tested on that validation set.\nThis process is repeated 5 times so that every fold (or subset) gets a turn as the validation set.\n\nWhy do we do this? Because it helps us avoid results that are just due to random chance from a single split. Finally, the validation results from all 5 folds are averaged, which gives us a more stable and reliable estimate of model performance before we even touch the test set.\n\nlm_model &lt;- train(\n  SalePrice ~ .,                \n  data = train_data,\n  method = \"lm\",          # this is where you change model type\n  trControl = trainControl(method = \"cv\", number = 5)\n)\n\nStep 4: Evaluate Model Accuracy Using RMSE\nNow we evaluate how well our model performs on unseen data, that’s what the test set is for.\n\n# use predict() to generate predicted sale prices for the test set.\npred &lt;- predict(lm_model, newdata = test_data) \n\n# compare those predictions to the actual sale prices using RMSE\nlm_result &lt;- RMSE(pred, test_data$SalePrice)\n\nlm_result\n\n[1] 269617\n\n\nRMSE is calculated by compare the predicted and actual values in the test data, square it, find the mean, then take the square root, you have do it by hand like the following code\n\nsqrt(mean((test_data$SalePrice - pred)^2))\n\n[1] 269617"
  },
  {
    "objectID": "labs/lab8.html#regularization-controlling-model-complexity",
    "href": "labs/lab8.html#regularization-controlling-model-complexity",
    "title": "Multivariate Regression and Model Selection with ",
    "section": "Regularization: Controlling Model Complexity",
    "text": "Regularization: Controlling Model Complexity\nIt might seem like a good idea to include as many variables as possible in our linear model, but that’s not always the best approach. When we include too many predictors, the model can start to overfit, meaning it fits the training data too closely and ends up capturing random noise rather than meaningful patterns.\nRegularization, in its broad sense, is a way to keep models simpler and more generalizable. For linear models, regularization helps the model to “selectively use” only the most important predictors.\nThere are two common types of linear model regularization: Ridge regression shrinks the coefficients of less important variables. Lasso regression shrinks some coefficients all the way to zero, effectively dropping those variables from the model.\nI’m repeating and putting the four steps together in the following code. The only difference is in the model-building step. Different models require their own set of tuning parameters, which we’ll explore more next week.\n\nmodeldata &lt;- boston |&gt; \n  select(-c(Parcel_No, PricePerSq, LU, R_BLDG_STY,\n                                  Latitude, Longitude))  |&gt; \n  filter(Style != \"Unknown\")|&gt; \n  filter(SalePrice &lt; 5000000)\n\nset.seed(42)  # for reproducibility\n\ntrain_index &lt;- createDataPartition(\n  y = modeldata$SalePrice,\n  p = 0.7,\n  list = FALSE\n)\n\ntrain_data &lt;- modeldata[ train_index,]\ntest_data  &lt;- modeldata[-train_index,]\n\nlasso_model &lt;- train(\n  SalePrice ~ ., \n  data = train_data,\n  trControl = trainControl(method = \"cv\", number = 5),\n  method = \"glmnet\",\n  tuneLength = 10,\n  preProcess = c(\"center\", \"scale\")\n)\n\n\npred &lt;- predict(lasso_model, newdata = test_data) \nlasso_result &lt;- RMSE(pred, test_data$SalePrice)\nlasso_result\n\n[1] 268423.9"
  },
  {
    "objectID": "labs/lab8.html#non-linear-model",
    "href": "labs/lab8.html#non-linear-model",
    "title": "Multivariate Regression and Model Selection with ",
    "section": "Non-linear model",
    "text": "Non-linear model\nSo far we have been working with linear models, But sometimes, the data just doesn’t fit a straight line (or a flat plane, if we’re in a space with many variables). Real-world relationships can be nonlinear, irregular, or involve interactions between variables that a linear model can’t easily capture.\nWe will introduce Random Forest model next time, but it is one of the machine learning models that do not assume a straight-line relationship between predictors and the outcome (non-linear model), and do not rely on a fixed number of parameters (non-parametric model). The model learns the structure from the data itself.\nHere is how we change the workflow to building a random forest model.\n\nset.seed(42)\nrf_model &lt;- train(\n  SalePrice ~ .,        \n  data = train_data,\n  method = \"rf\",        \n  trControl = trainControl(\"cv\", number = 5),\n  tuneLength = 5\n)\n\n\npred &lt;- predict(rf_model, newdata = test_data) \nrf_result &lt;- RMSE(pred, test_data$SalePrice)\nrf_result\n\n[1] 234362"
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lecture Overview",
    "section": "",
    "text": "Welcome to the lectures section. Click below to access individual sessions.\n\nLecture 1: Course Overview\nLecture 2: R Programming and Data Manipulation\nLecture 3: Data Transformation and Visualization I\nLecture 4: Data Transformation and Visualization II\nLecture 5: Spatial analysis with R\nLecture 6: Working with Census Data\nLecture 7: Interactive Map and Dashboard\nLecture 8: Web-based storytelling\nLecture 9: Machine Learning: Concepts & Workflow\nLecture 10: Geospatial ML and Hyperparameter Tuning\nLecture 11: Classification Models, Algorithmic Fairness\nLecture 12: Overview of Neural Networks"
  },
  {
    "objectID": "modules/index2.html",
    "href": "modules/index2.html",
    "title": "Module 2 Geospatial Data Science",
    "section": "",
    "text": "We’ll learn how to import, manipulate, and visualize geospatial datasets, from simple maps to more complex spatial analyses.\n\nSpatial analysis\nMapping census data"
  },
  {
    "objectID": "modules/index4.html",
    "href": "modules/index4.html",
    "title": "Module 4 Predictive Data Science",
    "section": "",
    "text": "Introduces modern machine learning methods. We’ll work on the skills to move from describing what has happened to predicting what is likely to happen next.\n\nMachine Learning: Concepts & Workflow\nML Models and Hyperparameter Tuning\nClassification, Fairness-aware ML\nOverview of Neural Networks"
  }
]